{
  
    
        "post0": {
            "title": "Background",
            "content": "Table of Contents . 1&nbsp;&nbsp;Background | 2&nbsp;&nbsp;Following The Guide from RealFaviconGenerator2.1&nbsp;&nbsp;Changing Path | 2.2&nbsp;&nbsp;Version/Refresh | 2.3&nbsp;&nbsp;Adjustments to RealFaviconGenerator&#39;s Instructions | . | 3&nbsp;&nbsp;Final Steps | .",
            "url": "https://progressedd.github.io/blog/2020/08/09/Favicon-Setup-Guide.html",
            "relUrl": "/2020/08/09/Favicon-Setup-Guide.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Using Matplotlib to Animate a Data From a Velocity Recording",
            "content": "Table of Contents . 1&nbsp;&nbsp;Using Matplotlib to Animate a Data From a Velocity Recording | 2&nbsp;&nbsp;Introduction | 3&nbsp;&nbsp;Demonstration3.1&nbsp;&nbsp;Tracking a Line Through Time | 3.2&nbsp;&nbsp;Plotting a Dot and Tracking Across Velocity Curve | 3.3&nbsp;&nbsp;Generating The Gifs | . | 4&nbsp;&nbsp;Importing Libraries + Loading Data | 5&nbsp;&nbsp;Plotting Function5.1&nbsp;&nbsp;Explanation of Inputs | 5.2&nbsp;&nbsp;Explanation of Variables5.2.1&nbsp;&nbsp;Setting Up Data Section | 5.2.2&nbsp;&nbsp;Graph Type Settings Section | 5.2.3&nbsp;&nbsp;Animation Settings | 5.2.4&nbsp;&nbsp;Setting Up The Figure | 5.2.5&nbsp;&nbsp;Setting Up Positions of X and Y Outputs For Value | 5.2.6&nbsp;&nbsp;Setting Up The Line Plot | 5.2.7&nbsp;&nbsp;Initialization Function: Plot The Background of Each Frame | 5.2.8&nbsp;&nbsp;Animation Function. This Is Called Sequentially | 5.2.9&nbsp;&nbsp;Save The Animation as an mp4. | . | . | 6&nbsp;&nbsp;Working Through Matplotlib Animation Tutorial6.1&nbsp;&nbsp;Having Line Plotted Through Time6.1.1&nbsp;&nbsp;Learning How Matplotlib plots | 6.1.2&nbsp;&nbsp;Messing with labels | 6.1.3&nbsp;&nbsp;print statements | . | 6.2&nbsp;&nbsp;Having a Dot Tracking Along the Velocity Curve | 6.3&nbsp;&nbsp;Figuring out Keyerror | . | 7&nbsp;&nbsp;Messing with a overlapping graph | . Introduction . One of my friends approached me asking if I had any experience animating graphs, since I had experience with video editing. Though I had not had any formal experience, I decided that it would be a interesting challenge to embark. . I did some research and found a tutorial as seen in the section &quot;Working Through Matplotlib Animation Tutorial&quot; . Demonstration . Tracking a Line Through Time . . Plotting a Dot and Tracking Across Velocity Curve . . Generating The Gifs . To generate these gifs, I used ffmpeg to convert the mp4s to gifs. . To convert the mp4s to gifs, I opened my directory in the terminal and ran the following ffmpeg command, after converting the video to gif, I added and pushed the files to github. Since github does not show local embeds, I embedded the url of the gif from the github repository. . ffmpeg -i line_tracking_animated.mp4 line_tracking_animated.gif . Alternatively, if you have ffmpeg installed, you can run the following cell to run it in a jupyter cell . Breakdown of the command . ffmpeg tells the terminal to use ffmpeg | -i tells ffmpeg the input | line_tracking_animated.mp4 tells ffmpeg the source file | line_tracking_animated.gif tells ffmpeg the output file title and format | . !ffmpeg -i line_tracking_animated.mp4 line_tracking_animated.gif . !ffmpeg -i dot_tracking_animated.mp4 dot_tracking_animated.gif . Importing Libraries + Loading Data . from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;&quot;)) . import pandas as pd import numpy as np from matplotlib import pyplot as plt from matplotlib import animation from matplotlib.offsetbox import AnchoredText . df = pd.read_csv(&quot;velocity.csv&quot;) df . Time (ms) Velocity (m/s) . 0 0 | 0.03 | . 1 17 | 0.03 | . 2 34 | 0.03 | . 3 50 | 0.03 | . 4 67 | 0.03 | . ... ... | ... | . 1314 22167 | 0.89 | . 1315 22183 | 0.89 | . 1316 22200 | 0.41 | . 1317 22217 | 0.41 | . 1318 22233 | 0.41 | . 1319 rows × 2 columns . print(&quot;max time&quot;) display(df[&quot;Time (ms)&quot;].max()) print(&quot;min time&quot;) display(df[&quot;Time (ms)&quot;].min()) print(&quot;Velocity&quot;) display(df[&quot;Velocity (m/s)&quot;][1]) print(&quot;Max Velocity&quot;) display(df[&quot;Velocity (m/s)&quot;].max()) print(&quot;Final time&quot;) display(df[&quot;Time (ms)&quot;].iloc[-1]*60/1000) print(&quot;Final time rounded&quot;) display(int(round(df[&quot;Time (ms)&quot;].iloc[-1]*60/1000, 1))) print(&quot;column titles&quot;) column_titles = list(df.columns.values) display(column_titles) . max time . 22233 . min time . 0 . Velocity . 0.03 . Max Velocity . 20.5 . Final time . 1333.98 . Final time rounded . 1334 . column titles . [&#39;Time (ms)&#39;, &#39;Velocity (m/s)&#39;] . Plotting Function . I wrote the animateGraph after modifying Matplotlib Animation Tutorial to work for my velocity data from the Having Line Plotted Through Time section and later Having a Dot Tracking Along the Velocity Curve. . The function takes the general structure of the respective and makes it into a single function . Explanation of Inputs . df input DataFrame | x_column column title for x axis | y_column column title for y axis | dot_track takes yes or no, if it is not yes, it will default to the line graph | framerate integer that specifies the video&#39;s framerate | . Explanation of Variables . Setting Up Data Section . x_df DataFrame of just the x values, I seperated the x and y DataFrames to make it easier to follow in the animation function | y_df DataFrame of just the x values, I seperated the x and y DataFrames to make it easier to follow in the animation function | x_max maximum of the x values, used to set the bounds of x axis | y_max maximum of the y values, used to set the bounds of y axis, added 2 to make space for the time and velocity printouts | last_index used to prevent Keyerror, see Figuring out Keyerror section for a more in depth explanation | . Graph Type Settings Section . plot_dot boolean used to decide which animation to use, if True will have animation output the dot tracker on graph | graph_type string used to keep track of the animation, used for filename can be either dot_tracking or line_tracking | . | . Animation Settings . time_seconds takes last entry of x value to get the duration of the data converts from miliseconds to seconds, used later for totalFrames animation duration calculation | totalFrames product of framerate and time_seconds to get the number of frames for matplotlib to animate | . Setting Up The Figure . fig figure variable, stores figure object the size was set to (20,12) | I followed the suggestions to change the size and output a sample file from Stack Overflow How to Change Figure Size | I followed the suggestions to change the font size from Stack Overflow How to Change Font Size | . | . Setting Up Positions of X and Y Outputs For Value . I followed the example set by the matplotlib wiki from Text properties and layout | y_value_print text object, will set the y value, for the demo, we use velocity set for right and top is updated frame by frame in the animate(i) function | x_value_print text object, will set the x value, for the demo, we use time set for left and top is updated frame by frame in the animate(i) function | . Setting Up The Line Plot . I used the plot_dot to determine which graph should be plotted if plot_dot is True, we will plot the existing graph and have the dot | if plot_dot is False, we will plot the graph sections frame by frame | . | To get the red dot, I read the instructions of the different passable arguments from the matplotlib wiki from matplotlib.pyplot.plot | . Initialization Function: Plot The Background of Each Frame . I kept the example set from the guide see Working Through Matplotlib Animation Tutorial for more details | . Animation Function. This Is Called Sequentially . This section is broken up into two parts, Checking if the function reaches the last value of the DataFrame | updating the x and y column print outs | | To check if the function has reached the last value of the DataFrame, we have a if conditional that checks if the animate function has reached the end of the DataFrame, if it does, then it will use the value at last_index, we want it to keep the last value to avoid the Keyerror as explained in the later experimental section | Next, the function will set the x_value and y_value to be the final index value x_value and y_value is used for animating the labels for the values of x and y | . | Afterwards the function follow a second if statement, which will check if plot_dot is true or false if plot_dot is False, the function will output everything up until the current i index value. This is used for the animation that plots graph sections frame by frame | . | if plot_dot is True, the function will output the current i index value. This is used for the animation that plots the point frame by frame | . | . | The next section will update the x and y column print out values for each frame I took inspiration from Matplotlib animations the easy way specifically the &quot;Changing labels and text&quot; section values_x is a string variable that stores the x column title, adds colons, and the x value | values_y is a string variable that stores the y column title, adds colons, and the y value | After setting values_x and values_y we use .set_text() to update the y_value_print and x_value_print each time | . | Once all the variables have been updated, animate(i) will return the line to the animation.FuncAnimation(), which will continue until it reaches the last frame | . Save The Animation as an mp4. . I did not make significant changes to the original, I only changed the filename to match the graph type. I wanted the filename to be the &#39; x_column vs y_column graph_type _animated.mp4&#39; | The ideal file name would be &#39;Velocity_(m/s)_vs_Time (ms)_line_tracking_animated.mp4 When I tried parsing the x_column as a string, python had serious issues with the slash (/) part of (m/s), enough so that it would prevent the file from being saved | An potential alternative solution would to change m/s to ms^-1, but it require changing the data or engineering a solution that read the units and replaced slashes with unit^-1. After evaluating the alternatives, I realized it would be easier to rename the file. If this function were used to generate hundreds of graphs from hundreds of source files, I would need to find a better solution. Since the purpose of this function is to make it easier to change between the dot and line tracking, I did not invest any further time into developing an alternative solution | . | . | . def animateGraph(df,x_column, y_column, dot_track, framerate): # Setting up data x_df = df[x_column] y_df = df[y_column] x_max = int(np.ceil(df[x_column].max())) y_max = int(np.ceil(df[y_column].max())) + 2 last_index = len(df)-1 # Check if we are plotting the graph or a graph + dot if(dot_track.lower() == &quot;yes&quot;): plot_dot = True graph_type = &quot;dot_tracking&quot; else: plot_dot = False graph_type = &quot;line_tracking&quot; # Animation settings time_seconds = x_df.iloc[-1]/1000 totalFrames = int(round(framerate * time_seconds)) # Setting up the figure fig = plt.figure() fig.set_size_inches(20, 12) fig.savefig(&#39;test2png.png&#39;, dpi=100) plt.rcParams.update({&#39;font.size&#39;: 24}) # Setting up the axes ax = plt.axes(xlim=(0, x_max), ylim=(0, y_max)) ax.set(title= x_column + &#39; vs &#39; + y_column, ylabel= y_column, xlabel= x_column, ) # Setting up the positions of velocity and time outputs left, width = .1, .75 bottom, height = .25, .73 right = left + width top = bottom + height y_value_print = ax.text(right, top, &quot;y value&quot;, horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;, transform=ax.transAxes) x_value_print = ax.text(left, top, &quot;x value&quot;, horizontalalignment=&#39;left&#39;, verticalalignment=&#39;top&#39;, transform=ax.transAxes) # Setting up the line plot if(plot_dot == True): plt.plot(x_df, y_df) line, = plt.plot([], [], linestyle=&#39;none&#39;, marker = &#39;o&#39;, ms = 10, color=&#39;r&#39;) else: line, = plt.plot(x_df, y_df) # initialization function: plot the background of each frame def init(): line.set_data([], []) return line, # animation function. This is called sequentially def animate(i): if(i &gt; last_index): x_value = x_df[last_index] y_value = y_df[last_index] if(plot_dot == False): x = x_df[0:last_index] y = y_df[0:last_index] else: x = x_df[last_index] y = y_df[last_index] else: x_value = x_df[i] y_value = y_df[i] if(plot_dot == False): x = x_df[0:i] y = y_df[0:i] else: x = x_df[i] y = y_df[i] line.set_data(x, y) # Update the figure with x and y values values_x = x_column + &quot;: &quot; + str(x_value) values_y = y_column + &quot;: &quot; + str(y_value) y_value_print.set_text(values_y) x_value_print.set_text(values_x) return line, # call the animator. blit=True means only re-draw the parts that have changed. anim = animation.FuncAnimation(fig, animate, init_func=init,frames = totalFrames, interval=0, blit=False) # save the animation as an mp4. This requires ffmpeg or mencoder to be # installed. The extra_args ensure that the x264 codec is used, so that # the video can be embedded in html5. You may need to adjust this for # your system: for more information, see # http://matplotlib.sourceforge.net/api/animation_api.html anim.save(graph_type + &#39;_animated.mp4&#39;, fps=framerate, extra_args=[&#39;-vcodec&#39;, &#39;libx264&#39;]) plt.show() . animateGraph(df,&quot;Time (ms)&quot;, &quot;Velocity (m/s)&quot;, &quot;yes&quot;, 60) . animateGraph(df,&quot;Time (ms)&quot;, &quot;Velocity (m/s)&quot;, &quot;no&quot;, 60) . Working Through Matplotlib Animation Tutorial . I used Jake Vanderplas&#39;s sine wave example posted below as a base to understand how matplotlib animates. I modified it to make it work with the velocity and time data I was given from my friend. . &quot;&quot;&quot; Matplotlib Animation Example author: Jake Vanderplas email: vanderplas@astro.washington.edu website: http://jakevdp.github.com license: BSD Please feel free to use and modify this, but keep the above information. Thanks! &quot;&quot;&quot; import numpy as np from matplotlib import pyplot as plt from matplotlib import animation # First set up the figure, the axis, and the plot element we want to animate fig = plt.figure() ax = plt.axes(xlim=(0, 2), ylim=(-2, 2)) line, = ax.plot([], [], lw=2) # initialization function: plot the background of each frame def init(): line.set_data([], []) return line, # animation function. This is called sequentially def animate(i): x = np.linspace(0, 2, 1000) y = np.sin(2 * np.pi * (x - 0.01 * i)) line.set_data(x, y) return line, # call the animator. blit=True means only re-draw the parts that have changed. anim = animation.FuncAnimation(fig, animate, init_func=init, frames=200, interval=20, blit=True) # save the animation as an mp4. This requires ffmpeg or mencoder to be # installed. The extra_args ensure that the x264 codec is used, so that # the video can be embedded in html5. You may need to adjust this for # your system: for more information, see # http://matplotlib.sourceforge.net/api/animation_api.html anim.save(&#39;basic_animation.mp4&#39;, fps=30, extra_args=[&#39;-vcodec&#39;, &#39;libx264&#39;]) plt.show() . Having Line Plotted Through Time . From the base I added my own modifications titled Tracking a Line Through time, which included . setting variables for velocity (y_df) and time (x_df) dataframes | setting maximums for x and y | setting a time_seconds variable that converts the time in milliseconds to seconds | setting a framerate variable that sets the framerate of the graph | setting a totalFrames variable that sets the duration of the animation | increasing the figure size from 64x64 to a bigger 18.5x10.4 inch higher resolution figure with fig.set_size_inches() | increasing the framerate to 60 frames a second (setting framerate to 60), which makes the plot smoother only downside is that it will take extra time to encode the final mp4 | . | modifying the limits. | adding titles for the figure | having the the animation plot the figure as time passes, rather needing a known function to plot it | . x_df = df[&quot;Time (ms)&quot;] y_df = df[&quot;Velocity&quot;] x_max = int(np.ceil(df[&quot;Time (ms)&quot;].max())) y_max = int(np.ceil(df[&quot;Velocity&quot;].max())) time_seconds = x_df.iloc[-1]/1000 framerate = 60 totalFrames = int(round(framerate * time_seconds)) # First set up the figure, the axis, and the plot element we want to animate fig = plt.figure() fig.set_size_inches(18.5, 10.5) fig.savefig(&#39;test2png.png&#39;, dpi=100) # transparency settings for the figure #fig.patch.set_alpha(0.) ax = plt.axes(xlim=(0, x_max), ylim=(0, y_max)) ax.set(title=&#39;Velocity vs Time&#39;, ylabel=&quot;Velocity&quot;, xlabel=&quot;Time (ms)&quot;) line, = plt.plot(x_df, y_df) # initialization function: plot the background of each frame def init(): line.set_data([], []) return line, # animation function. This is called sequentially def animate(i): x = x_df[0:i] print(&quot;i&quot;, i) y = y_df[0:i] line.set_data(x, y) return line, # call the animator. blit=True means only re-draw the parts that have changed. anim = animation.FuncAnimation(fig, animate, init_func=init,frames = totalFrames, interval=0, blit=False) # save the animation as an mp4. This requires ffmpeg or mencoder to be # installed. The extra_args ensure that the x264 codec is used, so that # the video can be embedded in html5. You may need to adjust this for # your system: for more information, see # http://matplotlib.sourceforge.net/api/animation_api.html anim.save(&#39;basic_animation.mp4&#39;, fps=framerate, extra_args=[&#39;-vcodec&#39;, &#39;libx264&#39;]) plt.show() . i 0 i 1 i 2 i 3 i 4 i 5 i 6 i 7 i 8 i 9 i 10 i 11 i 12 i 13 i 14 i 15 i 16 i 17 i 18 i 19 i 20 i 21 i 22 i 23 i 24 i 25 i 26 i 27 i 28 i 29 i 30 i 31 i 32 i 33 i 34 i 35 i 36 i 37 i 38 i 39 i 40 i 41 i 42 i 43 i 44 i 45 i 46 i 47 i 48 i 49 i 50 i 51 i 52 i 53 i 54 i 55 i 56 i 57 i 58 i 59 i 60 i 61 i 62 i 63 i 64 i 65 i 66 i 67 i 68 i 69 i 70 i 71 i 72 i 73 i 74 i 75 i 76 i 77 i 78 i 79 i 80 i 81 i 82 i 83 i 84 i 85 i 86 i 87 i 88 i 89 i 90 i 91 i 92 i 93 i 94 i 95 i 96 i 97 i 98 i 99 i 100 i 101 i 102 i 103 i 104 i 105 i 106 i 107 i 108 i 109 i 110 i 111 i 112 i 113 i 114 i 115 i 116 i 117 i 118 i 119 i 120 i 121 i 122 i 123 i 124 i 125 i 126 i 127 i 128 i 129 i 130 i 131 i 132 i 133 i 134 i 135 i 136 i 137 i 138 i 139 i 140 i 141 i 142 i 143 i 144 i 145 i 146 i 147 i 148 i 149 i 150 i 151 i 152 i 153 i 154 i 155 i 156 i 157 i 158 i 159 i 160 i 161 i 162 i 163 i 164 i 165 i 166 i 167 i 168 i 169 i 170 i 171 i 172 i 173 i 174 i 175 i 176 i 177 i 178 i 179 i 180 i 181 i 182 i 183 i 184 i 185 i 186 i 187 i 188 i 189 i 190 i 191 i 192 i 193 i 194 i 195 i 196 i 197 i 198 i 199 i 200 i 201 i 202 i 203 i 204 i 205 i 206 i 207 i 208 i 209 i 210 i 211 i 212 i 213 i 214 i 215 i 216 i 217 i 218 i 219 i 220 i 221 i 222 i 223 i 224 i 225 i 226 i 227 i 228 i 229 i 230 i 231 i 232 i 233 i 234 i 235 i 236 i 237 i 238 i 239 i 240 i 241 i 242 i 243 i 244 i 245 i 246 i 247 i 248 i 249 i 250 i 251 i 252 i 253 i 254 i 255 i 256 i 257 i 258 i 259 i 260 i 261 i 262 i 263 i 264 i 265 i 266 i 267 i 268 i 269 i 270 i 271 i 272 i 273 i 274 i 275 i 276 i 277 i 278 i 279 i 280 i 281 i 282 i 283 i 284 i 285 i 286 i 287 i 288 i 289 i 290 i 291 i 292 i 293 i 294 i 295 i 296 i 297 i 298 i 299 i 300 i 301 i 302 i 303 i 304 i 305 i 306 i 307 i 308 i 309 i 310 i 311 i 312 i 313 i 314 i 315 i 316 i 317 i 318 i 319 i 320 i 321 i 322 i 323 i 324 i 325 i 326 i 327 i 328 i 329 i 330 i 331 i 332 i 333 i 334 i 335 i 336 i 337 i 338 i 339 i 340 i 341 i 342 i 343 i 344 i 345 i 346 i 347 i 348 i 349 i 350 i 351 i 352 i 353 i 354 i 355 i 356 i 357 i 358 i 359 i 360 i 361 i 362 i 363 i 364 i 365 i 366 i 367 i 368 i 369 i 370 i 371 i 372 i 373 i 374 i 375 i 376 i 377 i 378 i 379 i 380 i 381 i 382 i 383 i 384 i 385 i 386 i 387 i 388 i 389 i 390 i 391 i 392 i 393 i 394 i 395 i 396 i 397 i 398 i 399 i 400 i 401 i 402 i 403 i 404 i 405 i 406 i 407 i 408 i 409 i 410 i 411 i 412 i 413 i 414 i 415 i 416 i 417 i 418 i 419 i 420 i 421 i 422 i 423 i 424 i 425 i 426 i 427 i 428 i 429 i 430 i 431 i 432 i 433 i 434 i 435 i 436 i 437 i 438 i 439 i 440 i 441 i 442 i 443 i 444 i 445 i 446 i 447 i 448 i 449 i 450 i 451 i 452 i 453 i 454 i 455 i 456 i 457 i 458 i 459 i 460 i 461 i 462 i 463 i 464 i 465 i 466 i 467 i 468 i 469 i 470 i 471 i 472 i 473 i 474 i 475 i 476 i 477 i 478 i 479 i 480 i 481 i 482 i 483 i 484 i 485 i 486 i 487 i 488 i 489 i 490 i 491 i 492 i 493 i 494 i 495 i 496 i 497 i 498 i 499 i 500 i 501 i 502 i 503 i 504 i 505 i 506 i 507 i 508 i 509 i 510 i 511 i 512 i 513 i 514 i 515 i 516 i 517 i 518 i 519 i 520 i 521 i 522 i 523 i 524 i 525 i 526 i 527 i 528 i 529 i 530 i 531 i 532 i 533 i 534 i 535 i 536 i 537 i 538 i 539 i 540 i 541 i 542 i 543 i 544 i 545 i 546 i 547 i 548 i 549 i 550 i 551 i 552 i 553 i 554 i 555 i 556 i 557 i 558 i 559 i 560 i 561 i 562 i 563 i 564 i 565 i 566 i 567 i 568 i 569 i 570 i 571 i 572 i 573 i 574 i 575 i 576 i 577 i 578 i 579 i 580 i 581 i 582 i 583 i 584 i 585 i 586 i 587 i 588 i 589 i 590 i 591 i 592 i 593 i 594 i 595 i 596 i 597 i 598 i 599 i 600 i 601 i 602 i 603 i 604 i 605 i 606 i 607 i 608 i 609 i 610 i 611 i 612 i 613 i 614 i 615 i 616 i 617 i 618 i 619 i 620 i 621 i 622 i 623 i 624 i 625 i 626 i 627 i 628 i 629 i 630 i 631 i 632 i 633 i 634 i 635 i 636 i 637 i 638 i 639 i 640 i 641 i 642 i 643 i 644 i 645 i 646 i 647 i 648 i 649 i 650 i 651 i 652 i 653 i 654 i 655 i 656 i 657 i 658 i 659 i 660 i 661 i 662 i 663 i 664 i 665 i 666 i 667 i 668 i 669 i 670 i 671 i 672 i 673 i 674 i 675 i 676 i 677 i 678 i 679 i 680 i 681 i 682 i 683 i 684 i 685 i 686 i 687 i 688 i 689 i 690 i 691 i 692 i 693 i 694 i 695 i 696 i 697 i 698 i 699 i 700 i 701 i 702 i 703 i 704 i 705 i 706 i 707 i 708 i 709 i 710 i 711 i 712 i 713 i 714 i 715 i 716 i 717 i 718 i 719 i 720 i 721 i 722 i 723 i 724 i 725 i 726 i 727 i 728 i 729 i 730 i 731 i 732 i 733 i 734 i 735 i 736 i 737 i 738 i 739 i 740 i 741 i 742 i 743 i 744 i 745 i 746 i 747 i 748 i 749 i 750 i 751 i 752 i 753 i 754 i 755 i 756 i 757 i 758 i 759 i 760 i 761 i 762 i 763 i 764 i 765 i 766 i 767 i 768 i 769 i 770 i 771 i 772 i 773 i 774 i 775 i 776 i 777 i 778 i 779 i 780 i 781 i 782 i 783 i 784 i 785 i 786 i 787 i 788 i 789 i 790 i 791 i 792 i 793 i 794 i 795 i 796 i 797 i 798 i 799 i 800 i 801 i 802 i 803 i 804 i 805 i 806 i 807 i 808 i 809 i 810 i 811 i 812 i 813 i 814 i 815 i 816 i 817 i 818 i 819 i 820 i 821 i 822 i 823 i 824 i 825 i 826 i 827 i 828 i 829 i 830 i 831 i 832 i 833 i 834 i 835 i 836 i 837 i 838 i 839 i 840 i 841 i 842 i 843 i 844 i 845 i 846 i 847 i 848 i 849 i 850 i 851 i 852 i 853 i 854 i 855 i 856 i 857 i 858 i 859 i 860 i 861 i 862 i 863 i 864 i 865 i 866 i 867 i 868 i 869 i 870 i 871 i 872 i 873 i 874 i 875 i 876 i 877 i 878 i 879 i 880 i 881 i 882 i 883 i 884 i 885 i 886 i 887 i 888 i 889 i 890 i 891 i 892 i 893 i 894 i 895 i 896 i 897 i 898 i 899 i 900 i 901 i 902 i 903 i 904 i 905 i 906 i 907 i 908 i 909 i 910 i 911 i 912 i 913 i 914 i 915 i 916 i 917 i 918 i 919 i 920 i 921 i 922 i 923 i 924 i 925 i 926 i 927 i 928 i 929 i 930 i 931 i 932 i 933 i 934 i 935 i 936 i 937 i 938 i 939 i 940 i 941 i 942 i 943 i 944 i 945 i 946 i 947 i 948 i 949 i 950 i 951 i 952 i 953 i 954 i 955 i 956 i 957 i 958 i 959 i 960 i 961 i 962 i 963 i 964 i 965 i 966 i 967 i 968 i 969 i 970 i 971 i 972 i 973 i 974 i 975 i 976 i 977 i 978 i 979 i 980 i 981 i 982 i 983 i 984 i 985 i 986 i 987 i 988 i 989 i 990 i 991 i 992 i 993 i 994 i 995 i 996 i 997 i 998 i 999 i 1000 i 1001 i 1002 i 1003 i 1004 i 1005 i 1006 i 1007 i 1008 i 1009 i 1010 i 1011 i 1012 i 1013 i 1014 i 1015 i 1016 i 1017 i 1018 i 1019 i 1020 i 1021 i 1022 i 1023 i 1024 i 1025 i 1026 i 1027 i 1028 i 1029 i 1030 i 1031 i 1032 i 1033 i 1034 i 1035 i 1036 i 1037 i 1038 i 1039 i 1040 i 1041 i 1042 i 1043 i 1044 i 1045 i 1046 i 1047 i 1048 i 1049 i 1050 i 1051 i 1052 i 1053 i 1054 i 1055 i 1056 i 1057 i 1058 i 1059 i 1060 i 1061 i 1062 i 1063 i 1064 i 1065 i 1066 i 1067 i 1068 i 1069 i 1070 i 1071 i 1072 i 1073 i 1074 i 1075 i 1076 i 1077 i 1078 i 1079 i 1080 i 1081 i 1082 i 1083 i 1084 i 1085 i 1086 i 1087 i 1088 i 1089 i 1090 i 1091 i 1092 i 1093 i 1094 i 1095 i 1096 i 1097 i 1098 i 1099 i 1100 i 1101 i 1102 i 1103 i 1104 i 1105 i 1106 i 1107 i 1108 i 1109 i 1110 i 1111 i 1112 i 1113 i 1114 i 1115 i 1116 i 1117 i 1118 i 1119 i 1120 i 1121 i 1122 i 1123 i 1124 i 1125 i 1126 i 1127 i 1128 i 1129 i 1130 i 1131 i 1132 i 1133 i 1134 i 1135 i 1136 i 1137 i 1138 i 1139 i 1140 i 1141 i 1142 i 1143 i 1144 i 1145 i 1146 i 1147 i 1148 i 1149 i 1150 i 1151 i 1152 i 1153 i 1154 i 1155 i 1156 i 1157 i 1158 i 1159 i 1160 i 1161 i 1162 i 1163 i 1164 i 1165 i 1166 i 1167 i 1168 i 1169 i 1170 i 1171 i 1172 i 1173 i 1174 i 1175 i 1176 i 1177 i 1178 i 1179 i 1180 i 1181 i 1182 i 1183 i 1184 i 1185 i 1186 i 1187 i 1188 i 1189 i 1190 i 1191 i 1192 i 1193 i 1194 i 1195 i 1196 i 1197 i 1198 i 1199 i 1200 i 1201 i 1202 i 1203 i 1204 i 1205 i 1206 i 1207 i 1208 i 1209 i 1210 i 1211 i 1212 i 1213 i 1214 i 1215 i 1216 i 1217 i 1218 i 1219 i 1220 i 1221 i 1222 i 1223 i 1224 i 1225 i 1226 i 1227 i 1228 i 1229 i 1230 i 1231 i 1232 i 1233 i 1234 i 1235 i 1236 i 1237 i 1238 i 1239 i 1240 i 1241 i 1242 i 1243 i 1244 i 1245 i 1246 i 1247 i 1248 i 1249 i 1250 i 1251 i 1252 i 1253 i 1254 i 1255 i 1256 i 1257 i 1258 i 1259 i 1260 i 1261 i 1262 i 1263 i 1264 i 1265 i 1266 i 1267 i 1268 i 1269 i 1270 i 1271 i 1272 i 1273 i 1274 i 1275 i 1276 i 1277 i 1278 i 1279 i 1280 i 1281 i 1282 i 1283 i 1284 i 1285 i 1286 i 1287 i 1288 i 1289 i 1290 i 1291 i 1292 i 1293 i 1294 i 1295 i 1296 i 1297 i 1298 i 1299 i 1300 i 1301 i 1302 i 1303 i 1304 i 1305 i 1306 i 1307 i 1308 i 1309 i 1310 i 1311 i 1312 i 1313 i 1314 i 1315 i 1316 i 1317 i 1318 i 1319 i 1320 i 1321 i 1322 i 1323 i 1324 i 1325 i 1326 i 1327 i 1328 i 1329 i 1330 i 1331 i 1332 i 1333 . Learning How Matplotlib plots . In the following cells, I experimented with the ranges for which data to be plotted. Once I figured out putting start:end into a dataframe, I used this for the first function . With an understanding of this property I was able to have the graph from 0 to the value at the specific frame. . #plt.scatter(x_df,y_df) plt.plot(x_df[0:100], y_df[0:100]) plt.show() . Messing with labels . When I intially made the velocity and time printouts, I thought about using AnchoredText boxes and having each frame update the boxes. Using this method was not the best as it made the processing of the animation take longer and the text boxes would not overlap. Later I learned how to animate text. . Although not shown here, I later found &lt;a href = https://brushingupscience.com/2016/06/21/matplotlib-animations-the-easy-way/&gt; a very helpful guide that demonstrated animating text and labels &lt;/a&gt;. I did not know that I could incoorperate text changes in the animate fucntion with on matplotlib. I implemented most of the changes of set_text() and ax.text() in the animateGraph() section. It was easier to modify animateGraph() as I could run it to get a output and would not have the same code in 3 other places of the notebook. . Another goal of this section was to experiment with matplotlib to get the outputs to my liking. I learned &lt;a href =https://stackoverflow.com/questions/332289/how-do-you-change-the-size-of-figures-drawn-with-matplotlib&gt; I could change the graph size with&lt;/a&gt; fig.set_size_inches() . fig = plt.figure() fig.set_size_inches(18.5, 10.5) fig.savefig(&#39;test2png.png&#39;, dpi=100) ax = plt.axes(xlim=(0, x_max), ylim=(0, y_max+2)) ax.set(title=&#39;Velocity vs Time&#39;, ylabel=&quot;Velocity&quot;, xlabel=&quot;Time (ms)&quot;) at1 = AnchoredText(x_df[100], prop=dict(size=15), frameon=True, loc=&#39;upper left&#39;, ) at.patch.set_boxstyle(&quot;round,pad=0.3,rounding_size=0.5&quot;) ax.add_artist(at1) at2 = AnchoredText(y_df[100], prop=dict(size=15), frameon=True, loc=&#39;upper right&#39;, ) at2.patch.set_boxstyle(&quot;round,pad=0.3,rounding_size=0.2&quot;) ax.add_artist(at2) at3 = AnchoredText(1.3, prop=dict(size=15), frameon=True, loc=&#39;upper right&#39;, ) at3.patch.set_boxstyle(&quot;round,pad=0.3,rounding_size=0.2&quot;) ax.add_artist(at3) plt.plot(x_df[0:100], y_df[0:100]) plt.show() . print statements . I used these print statements to figure out why my maxes were not being rounded to the nearest whole number. . Initially, I tried using round( #number, #decimal places), from Stackoverflow but the native function concatenated for values such as 20.5 to 20 instead of 21 . Eventually I found a detailed explanation behind the native python round() function from RealPython, in addition to suggested alternative functions and libraries I could use that would solve the issue I was running into. The suggestion was to use ceil from the math libraries. I ended up using ceil from the numpy libraries since I didn&#39;t want to import another library . print(&quot;y_max:&quot;, df[&quot;Time (ms)&quot;].max()) print() print(&quot;y_max ceil rounding:&quot;, np.ceil(df[&quot;Time (ms)&quot;].max())) print() print(&quot;x_max:&quot;, df[&quot;Velocity&quot;].max()) print() print(&quot;x_max ceil rounding:&quot;, np.ceil(df[&quot;Velocity&quot;].max())) print() print(&quot;issue rounded y_max (Time):&quot;, int(round(df[&quot;Time (ms)&quot;].max(),1))) print() print(&quot;issue rounded x_max (Velocity):&quot;, int(round(df[&quot;Velocity&quot;].max(),1))) print() . Having a Dot Tracking Along the Velocity Curve . After making my modifications, I copied my modified version and further adjusted it further titled Having a Dot Tracking Along the Velocity Curve to print the plot first and have a single point track along the plotted graph . Changes from the original Having Line Plotted Through time . added red dot for tracking instead of line used following additional arguments to set the dot linestyle=&#39;none&#39;, marker = &#39;o&#39;, ms = 10, color=&#39;r&#39;, I learned of these parameters from adrian prince-whelan&#39;s demonstration of Making a Matplotlib animation with a transparent background | linestyle=&#39;none&#39; prevents lines from being drawn | marker = &#39;o&#39; sets the dot | ms = 10 sets the dot size | color=&#39;r&#39; sets the dot color | . | messing with video transparency as well from the same post main issue right now is getting the correct save settings | . | changed x linspace value to have the previous frame added conditional where at i=0, i would start at 0 instead of -1 when calling the x_df[i-1] and x_df[i] dataframe entry | . | . x_df = df[&quot;Time (ms)&quot;] y_df = df[&quot;Velocity (m/s)&quot;] x_max = int(np.ceil(df[&quot;Time (ms)&quot;].max())) y_max = int(np.ceil(df[&quot;Velocity (m/s)&quot;].max())) time_seconds = x_df.iloc[-1]/1000 framerate = 60 totalFrames = len(df) # First set up the figure, the axis, and the plot element we want to animate fig = plt.figure() fig.set_size_inches(18.5, 10.5) fig.savefig(&#39;test2png.png&#39;, dpi=100) # transparency settings for the figure #fig.patch.set_alpha(0.) ax = plt.axes(xlim=(0, x_max), ylim=(0, y_max)) ax.set(title=&#39;Velocity vs Time&#39;, ylabel=&quot;Velocity&quot;, xlabel=&quot;Time (ms)&quot;) # transparency settings for the plot area #ax.patch.set_facecolor(&#39;#ababab&#39;) #ax.patch.set_alpha(0) plt.plot(x_df, y_df) line, = plt.plot([], [], linestyle=&#39;none&#39;, marker = &#39;o&#39;, ms = 10, color=&#39;r&#39;) # initialization function: plot the background of each frame def init(): line.set_data([], []) return line, # animation function. This is called sequentially def animate(i): x = x_df[i] #print(&quot;x&quot;) #print(x) y = y_df[i] line.set_data(x, y) return line, # call the animator. blit=True means only re-draw the parts that have changed. anim = animation.FuncAnimation(fig, animate, init_func=init,frames = totalFrames, interval=0, blit=True) # save the animation as an mp4. This requires ffmpeg or mencoder to be # installed. The extra_args ensure that the x264 codec is used, so that # the video can be embedded in html5. You may need to adjust this for # your system: for more information, see # http://matplotlib.sourceforge.net/api/animation_api.html anim.save(&#39;dot_tracking_animation_test.mp4&#39;, fps=framerate, extra_args=[&#39;-vcodec&#39;, &#39;libx264&#39;]) plt.show() . Figuring out Keyerror . While running Dot Tracking Along the Velocity Curve I got a consistent keyerror when plotting i. The following code blocks helped me debug the values of i for each iteration. . I later realized that the keyerror was caused by my totalFrames calculation, which took the last value of the data (milliseconds converted to seconds), multiplied it by the framerate, and was rounded to get total number of frames. Before changing to use the length of the dataframe, the totalFrames was 1333.98, which was rounded to 1334. The keyerror occured because animate(i) uses the frames variable as a index. Once animate reached the length of the dataframe (1319) the animation stopped, but in the animation.FuncAnimation() function continued to pass values in for i. When computing for x, this caused a keyerror since animation.FuncAnimation() would continue to feed values in for i, but in the dataframe no such values existed. . To prevent this from happening, I added a conditional that when we reached the end of the dataframe, we would use the last value of the dataframe. . x_df = df[&quot;Time (ms)&quot;] y_df = df[&quot;Velocity (m/s)&quot;] for i in x_df: x = x_df[i] #print(&quot;i = &quot;,i,&quot;,&quot;, &quot;a = &quot;,a) y = y_df[i] print(&quot;i = &quot;, i, &quot;x = &quot;, x, &quot;,&quot;, &quot;y = &quot;,y) print() . i = 0 x = 0 , y = 0.03 i = 17 x = 284 , y = 0.03 i = 34 x = 567 , y = 2.67 i = 50 x = 834 , y = 1.3 i = 67 x = 1134 , y = 10.33 i = 84 x = 1467 , y = 19.06 i = 100 x = 1734 , y = 20.01 i = 117 x = 2017 , y = 20.01 i = 134 x = 2300 , y = 6.73 i = 150 x = 2567 , y = 1.14 i = 167 x = 2850 , y = 0.0 i = 184 x = 3134 , y = 0.0 i = 200 x = 3400 , y = 0.0 i = 217 x = 3684 , y = 0.0 i = 234 x = 3967 , y = 4.01 i = 250 x = 4234 , y = 10.0 i = 267 x = 4517 , y = 3.24 i = 284 x = 4800 , y = 6.94 i = 300 x = 5067 , y = 4.32 i = 317 x = 5400 , y = 6.66 i = 334 x = 5684 , y = 3.74 i = 350 x = 5950 , y = 5.6 i = 367 x = 6234 , y = 4.38 i = 384 x = 6517 , y = 10.64 i = 400 x = 6784 , y = 15.76 i = 417 x = 7067 , y = 20.07 i = 434 x = 7350 , y = 20.09 i = 450 x = 7617 , y = 0.45 i = 467 x = 7900 , y = 4.64 i = 484 x = 8184 , y = 10.0 i = 500 x = 8450 , y = 10.0 i = 517 x = 8733 , y = 10.0 i = 534 x = 9017 , y = 10.0 i = 550 x = 9283 , y = 6.32 i = 567 x = 9567 , y = 8.59 i = 584 x = 9850 , y = 2.56 i = 600 x = 10117 , y = 3.77 i = 617 x = 10400 , y = 15.77 i = 634 x = 10683 , y = 20.17 i = 650 x = 10950 , y = 20.5 i = 667 x = 11233 , y = 15.99 i = 684 x = 11517 , y = 13.51 i = 700 x = 11783 , y = 10.74 i = 717 x = 12067 , y = 8.74 i = 734 x = 12350 , y = 5.64 i = 750 x = 12617 , y = 8.87 i = 767 x = 12900 , y = 13.08 i = 784 x = 13184 , y = 2.68 i = 800 x = 13450 , y = 0.01 i = 817 x = 13734 , y = 2.56 i = 834 x = 14017 , y = 10.0 i = 850 x = 14284 , y = 10.01 i = 867 x = 14567 , y = 10.0 i = 884 x = 14850 , y = 10.0 i = 900 x = 15117 , y = 10.01 i = 917 x = 15400 , y = 10.0 i = 934 x = 15684 , y = 10.0 i = 967 x = 16234 , y = 10.0 i = 984 x = 16517 , y = 10.02 i = 1000 x = 16784 , y = 13.24 i = 1017 x = 17117 , y = 20.0 i = 1034 x = 17400 , y = 18.88 i = 1050 x = 17667 , y = 18.0 i = 1067 x = 17950 , y = 18.0 i = 1084 x = 18234 , y = 18.01 i = 1100 x = 18500 , y = 18.28 i = 1117 x = 18784 , y = 18.09 i = 1134 x = 19067 , y = 17.36 i = 1150 x = 19334 , y = 17.2 i = 1167 x = 19717 , y = 17.59 i = 1184 x = 20000 , y = 15.97 i = 1200 x = 20267 , y = 15.02 i = 1217 x = 20550 , y = 15.48 i = 1234 x = 20833 , y = 15.09 i = 1250 x = 21100 , y = 15.05 i = 1267 x = 21383 , y = 3.15 i = 1284 x = 21667 , y = 8.25 i = 1300 x = 21933 , y = 5.85 i = 1317 x = 22217 , y = 0.41 . KeyError Traceback (most recent call last) &lt;ipython-input-9-9d1447f8ceb4&gt; in &lt;module&gt; 3 4 for i in x_df: -&gt; 5 x = x_df[i] 6 #print(&#34;i = &#34;,i,&#34;,&#34;, &#34;a = &#34;,a) 7 y = y_df[i] ~/miniconda3/lib/python3.7/site-packages/pandas/core/series.py in __getitem__(self, key) 1069 key = com.apply_if_callable(key, self) 1070 try: -&gt; 1071 result = self.index.get_value(self, key) 1072 1073 if not is_scalar(result): ~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_value(self, series, key) 4728 k = self._convert_scalar_indexer(k, kind=&#34;getitem&#34;) 4729 try: -&gt; 4730 return self._engine.get_value(s, k, tz=getattr(series.dtype, &#34;tz&#34;, None)) 4731 except KeyError as e1: 4732 if len(self) &gt; 0 and (self.holds_integer() or self.is_boolean()): pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_value() pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_value() pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item() pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item() KeyError: 1334 . df.iloc[1318] #len(df) . Time (ms) 22233.00 Velocity 0.41 Name: 1318, dtype: float64 . display(x_df) . display(framerate * time_seconds) totalFrames . 1333.98 . 1334 . Messing with a overlapping graph . while looking for solutions to animate the velocity and time outputs, I stumbled upon a method to use subplots. If I come back to this project, I would want to make a line tracing option, where a line plot is generated, and another plot will overlap . fig = plt.figure() fig.set_size_inches(18.5, 10.5) fig.savefig(&#39;test2png.png&#39;, dpi=100) fig, ax = plt.subplots() ax.plot(x_df, y_df) plt.plot(x_df[0:100], y_df[0:100]) plt.show() . !jupyter nbconvert Animating-Velocity-Graph.ipynb --to html . [NbConvertApp] Converting notebook Animating-Velocity-Graph.ipynb to html [NbConvertApp] Writing 812059 bytes to Animating-Velocity-Graph.html .",
            "url": "https://progressedd.github.io/blog/2020/07/02/Animating-Velocity-Graph.html",
            "relUrl": "/2020/07/02/Animating-Velocity-Graph.html",
            "date": " • Jul 2, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Generating The Total Nutritional Content For a Week",
            "content": "Table of Contents . 1&nbsp;&nbsp;Generating The Total Nutritional Content For a Week | 2&nbsp;&nbsp;Introduction | 3&nbsp;&nbsp;Loading The Libraries and Dataframes | 4&nbsp;&nbsp;Investigating Dataframes | 5&nbsp;&nbsp;Generating The Code5.1&nbsp;&nbsp;Variables Intialized outside of the loop | 5.2&nbsp;&nbsp;Explanation For The First loop | 5.3&nbsp;&nbsp;Explanation For The Second Loop | 5.4&nbsp;&nbsp;Explanation For The Third Loop | . | 6&nbsp;&nbsp;Generating The Summary | 7&nbsp;&nbsp;Tests Cells | . Introduction . During a conversation with one of my friends, I showed them some of my work with analyzing the protein content of various meats and vegetables. After seeing the analysis, they asked me if I would be interested in analyzing the total caloric intake for a week&#39;s worth of meals. . I learned a great amount concatenating, droping, and manipulating indexes in Pandas. . Loading The Libraries and Dataframes . I was given the following data from a single file, the original file had the tables in seperate sheets. I exported the original file into seperate csv files so that I could load each table into separate dataframes . import pandas as pd . from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;&quot;)) . df_days = pd.read_csv(&#39;Days.csv&#39;) df_ingredients = pd.read_csv(&#39;Ingredients.csv&#39;) df_meals = pd.read_csv(&#39;Meals.csv&#39;) df_results = pd.read_csv(&#39;Results.csv&#39;) . Investigating Dataframes . Before calculating the totals, I wanted to take a look at the dataframes, I transposed the columns in libreoffice such that the Days were columns, and the meals were rows. The majority of Pandas operations manipulate columns rather than rows. . I also transposed the columns for the other dataframes (df_meals and df_ingredients, )as well . df_days . Meal Monday Tuesday Wednesday Thursday Friday Saturday Sunday . 0 Standard breakfast | 1 | 1 | 1 | 0 | 0 | 0 | 0 | . 1 Standard lunch | 1 | 1 | 1 | 1 | 1 | 0 | 0 | . 2 Standard dinner | 1 | 1 | 1 | 0 | 0 | 0 | 0 | . 3 Weekday egg muffin breakfast | 0 | 0 | 0 | 1 | 1 | 0 | 0 | . 4 Weekend egg muffin breakfast | 0 | 0 | 0 | 0 | 0 | 1 | 2 | . 5 Seafood dinner | 0 | 0 | 0 | 0 | 0 | 1 | 1 | . 6 Pasta lunch | 0 | 0 | 0 | 0 | 0 | 1 | 1 | . 7 Rice and pinto beans dinner | 0 | 0 | 0 | 1 | 1 | 0 | 0 | . 8 Mango dessert | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . 9 Vanilla soymilk drink | 1 | 1 | 1 | 1 | 1 | 0 | 0 | . df_meals . Meal Standard breakfast Standard lunch Standard dinner Weekday egg muffin breakfast Weekend egg muffin breakfast Seafood dinner Pasta lunch Rice and pinto beans dinner Mango dessert Vanilla soymilk drink . 0 Brown rice | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 Whole wheat bread | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 Whole wheat cereal | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 Whole wheat muffin | 0 | 0 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | . 4 Whole wheat pasta | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 Boneless skinless chicken breast | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 Salmon | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 7 Pinto beans | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 8 Black beans | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 Peanut butter | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 10 Eggs | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | . 11 Vanilla soymilk | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 12 Unsweet soymilk | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 13 Spinach | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 14 Broccoli | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 15 Carrots | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 16 Tomatoes | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 17 Banana | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | . 18 Strawberries | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 19 Mangos | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 20 Strawberry jam | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 21 Marinara sauce | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 22 American cheese | 0 | 0 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | . 23 Olive oil | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 24 Salt | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . df_ingredients . Item Brand and style Quantity Calories Total Fat Saturated Fat Monounsaturated Fat Polyunsaturated Fat Cholesterol Carbs Sugar Protein Fiber Sodium . 0 Brown rice | 365 Everyday Value, long grain | 150g | 570 | 4.5 | 0.0 | 0.0 | 0.0 | 0 | 117 | 0 | 12 | 6 | 0 | . 1 Whole wheat bread | Brownberry, whole grains | 2 slices | 220 | 4.0 | 0.0 | 0.0 | 2.0 | 0 | 42 | 6 | 8 | 6 | 320 | . 2 Whole wheat cereal | 365 Everyday Value, wheat squares | 60g | 220 | 1.0 | 0.0 | 0.0 | 0.0 | 0 | 47 | 0 | 7 | 5 | 5 | . 3 Whole wheat muffin | Aunt Millie&#39;s, english muffins | 1 muffin | 110 | 1.5 | 0.0 | 0.0 | 0.5 | 0 | 21 | 1 | 6 | 3 | 180 | . 4 Whole wheat pasta | Barilla, whole grain penne | 4oz | 360 | 3.0 | 0.0 | 0.0 | 0.0 | 0 | 78 | 2 | 16 | 14 | 0 | . 5 Boneless skinless chicken breast | NaN | 3oz | 135 | 3.0 | 1.0 | 1.0 | 0.5 | 100 | 0 | 0 | 28 | 0 | 40 | . 6 Salmon | NaN | 6oz | 270 | 9.0 | 1.5 | 3.0 | 1.5 | 95 | 0 | 0 | 43 | 0 | 690 | . 7 Pinto beans | 365 Everyday Value | 455g | 350 | 0.0 | 0.0 | 0.0 | 0.0 | 0 | 67 | 2 | 25 | 28 | 18 | . 8 Black beans | 365 Everyday Value | 455g | 385 | 0.0 | 0.0 | 0.0 | 0.0 | 0 | 74 | 3 | 25 | 25 | 35 | . 9 Peanut butter | Jif, creamy | 33g | 190 | 16.0 | 3.5 | 0.0 | 0.0 | 0 | 8 | 3 | 7 | 2 | 140 | . 10 Eggs | Grade A large brown | 2 eggs | 180 | 14.0 | 4.0 | 6.0 | 3.0 | 370 | 1 | 0 | 12 | 0 | 190 | . 11 Vanilla soymilk | Silk, vanilla | 1 cup | 100 | 3.5 | 0.5 | 0.5 | 2.0 | 0 | 11 | 9 | 6 | 1 | 85 | . 12 Unsweet soymilk | Silk, unsweet vanilla | 1 cup | 80 | 4.0 | 0.5 | 1.0 | 2.5 | 0 | 4 | 1 | 7 | 2 | 70 | . 13 Spinach | NaN | 60g | 15 | 0.0 | 0.0 | 0.0 | 0.0 | 0 | 2 | 0 | 2 | 1 | 50 | . 14 Broccoli | Essential Everyday, frozen | 2 cups | 60 | 0.0 | 0.0 | 0.0 | 0.0 | 0 | 8 | 4 | 4 | 4 | 40 | . 15 Carrots | Green Giant, baby cut Carrots | 4.5oz | 55 | 0.0 | 0.0 | 0.0 | 0.0 | 0 | 12 | 8 | 1 | 3 | 98 | . 16 Tomatoes | Garden Ripe, grape tomatoes | 100g | 95 | 0.5 | 0.0 | 0.0 | 0.0 | 0 | 6 | 0 | 1 | 2 | 6 | . 17 Banana | NaN | 1 banana | 110 | 0.0 | 0.0 | 0.0 | 0.0 | 0 | 29 | 15 | 1 | 3 | 1 | . 18 Strawberries | Nature Blessed, frozen | 1 cup | 45 | 0.0 | 0.0 | 0.0 | 0.0 | 0 | 12 | 6 | 0 | 3 | 3 | . 19 Mangos | Nature Blessed, frozen | 1 cup | 75 | 0.0 | 0.0 | 0.0 | 0.0 | 0 | 20 | 19 | 1 | 1 | 1 | . 20 Strawberry jam | Smucker&#39;s, squeeze | 20g | 50 | 0.0 | 0.0 | 0.0 | 0.0 | 0 | 13 | 12 | 0 | 0 | 0 | . 21 Marinara sauce | Ragu Simply, chunky | 0.25 cups | 35 | 1.0 | 0.0 | 0.0 | 0.0 | 0 | 6 | 4 | 1 | 1 | 245 | . 22 American cheese | Dutch Farms, american singles | 1 slice | 70 | 5.0 | 0.0 | 0.0 | 0.0 | 0 | 2 | 0 | 3 | 0 | 300 | . 23 Olive oil | 365 Everyday Value, extra virgin | 1 tbsp | 120 | 14.0 | 2.0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | . 24 Salt | Morton, coarse kosher | 0.25 tsp | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 480 | . df_results . Day Calories Total Fat Saturated Fat Monounsaturated Fat Polyunsaturated Fat Cholesterol Carbs Sugar Protein Fiber Sodium . 0 Monday | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 Tuesday | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 Wednesday | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 Thursday | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 Friday | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 Saturday | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 6 Sunday | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . Generating The Code . After looking at the raw data and transposing the data, I decided to use multiple for loops to solve the problem. . My general plan followed . get meals for each day make a dataframe for each day where there is 1 | . | get foods for each meal | add each food to a list | add each ingredient to a list | store sum in results | The following code iterates through all the dataframesdf_days,df_meals, and df_ingredients intialized in the previous step to generate a dataframe with all the individual ingredients for all the days . Variables Intialized outside of the loop . meal_list, food_list, and ingredient_list are initially empty, we will set values for them once the loop begins. | days is a list of the days of the week, we will use to keep track of the day. Furthermore, we will use days to specify which column we want python to focus on | df_summary is a empty dataframe we will append the ingredients to | counter is a integer variable that that is used as a index when iterating the days list, it is set to be -1 to offset the starting column of df_days | . all print() and display() statements were used to view the variables as the loop iterated . Explanation For The First loop . The first for loop iterates accross the columns of df_days dataframe in the first loop, generates a list of meals consumed in a day . column_title is the title of the column from df_days | day is the current day | df_day_non_zero is df_days, but all values greater than 0 | meal_list uses df_day_non_zero but has only the meals quantity for a single day, .reset_index() resets the index as filtering for the day changes the index, .drop(columns = &quot;index&quot;) drops the index created by .reset_index() | . Explanation For The Second Loop . The second for loop iterates accross the columns of meal_list dataframe generated for a day from the previous step, generates a list of foods consumed in meal . meal_item is the title of the column from df_meals | df_empty is a empty temporary dataframe that we generate each iteration | food_quantity stores the number of meal consumed in a day | df_meal_day is df_meals, but all values greater than 0 | food_list dataframe that uses df_meal_day to get the meal item for a particular meal | food_list[meal_item] dataframe that multiplies the specific quantity by the food_quantity | . Explanation For The Third Loop . The third for loop iterates through the food_list dataframe generated for a meal from the previous step, concatenates the ingredients to df_empty dataframe, uses a conditional that will multiply the nutritional value by the number of values consumed . df_current_ingredient is df_ingredients but when the food item matches the value from `food_list | ingredient_quantity stores the number of meal items consumed in a single meal | . After the third for loop, we will concatenate all the ingredients per meal into a single dataframe with two columns for the day and meal the item was consumed from. When creating new columns using the df[&quot;new column&quot;] = value Pandas will add the new column to the end of the columns. . meal_list = [] food_list = [] ingredient_list = [] days = [&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;, &quot;Saturday&quot;, &quot;Sunday&quot;] df_summary = pd.DataFrame() counter = -1 # loop through days table for column_title in df_days: # get meals for each day # make a dataframe for each day where there is 1 day = days[counter] df_day_non_zero = df_days[df_days[day] &gt; 0] meal_list = df_day_non_zero[[&quot;Meal&quot;,day]].reset_index().drop(columns = &quot;index&quot;) #print statements for debugging and tracking variable values #meal_quantity = meal_list[day][] #print(&quot;days[i]&quot;) #display(days[counter]) #print(&quot;df_day&quot;) #display(df_day) #print(&quot;column_title&quot;) #display(column_title) #print(&quot;meal_list&quot;) #display(meal_list) food_counter = 0 # get foods for each meal for meal_item in meal_list[&quot;Meal&quot;]: #print(&quot;meal_item&quot;) #display(meal_item) df_empty = pd.DataFrame() food_quantity = meal_list[day][food_counter] #print(&quot;food_quantity multiplication factor&quot;) #display(food_quantity) df_meal_day = df_meals[df_meals[meal_item] &gt;= 1] food_list = df_meal_day[[&quot;Meal&quot;, meal_item]].reset_index().drop(columns = &quot;index&quot;) #print(&quot;food_list before multiplication&quot;) #display(food_list) food_list[meal_item] = food_list[meal_item]*food_quantity # multiplies the list by the number of times from days ie if someone eats 2 meals, the meal item will be multipled by 2 #print(&quot;food_list after multiplication&quot;) #display(food_list) # add each food to a list for food_list_index in range(len(food_list)): #print(&quot;food_list&quot;) #display(food_list) #print(&quot;food_list_index&quot;) #display(food_list_index) df_current_ingredient = df_ingredients[df_ingredients[&quot;Item&quot;] == food_list[&quot;Meal&quot;][food_list_index]] ingredient_quantity = food_list[meal_item][food_list_index] #print(&quot;df_current_ingredient&quot;) #display(df_current_ingredient) #print(&quot;food_list[Meal][food_list_index]&quot;) #display(food_list[&quot;Meal&quot;][food_list_index]) #print(&quot;ingredient_quantity&quot;) #display(ingredient_quantity) if(ingredient_quantity&gt;=1): df_empty = pd.concat([df_empty, df_current_ingredient * ingredient_quantity]) else: df_empty = pd.concat([df_empty, df_current_ingredient]) #display(df_empty) # add each ingredient #need to make a running total, and then append to a seperate df for each day df_empty.loc[column_title,:] = df_empty[[&quot;Item&quot;,&quot;Calories&quot;, &quot;Total Fat&quot;, &quot;Saturated Fat&quot;, &quot;Monounsaturated Fat&quot;, &quot;Polyunsaturated Fat&quot;, &quot;Cholesterol&quot;, &quot;Carbs&quot;, &quot;Sugar&quot;,&quot;Protein&quot;, &quot;Fiber&quot;, &quot;Sodium&quot;]].sum(axis=0) df_empty[&quot;Meal&quot;] = meal_item #display(df_empty) df_summary = pd.concat([df_summary, df_empty.tail(1)]) #display(df_summary) food_counter+=1 counter+=1 # store sum in results df_summary = df_summary.reset_index().rename(columns = {&quot;index&quot; : &quot;Day&quot;}) df_summary . Day Item Brand and style Quantity Calories Total Fat Saturated Fat Monounsaturated Fat Polyunsaturated Fat Cholesterol Carbs Sugar Protein Fiber Sodium Meal . 0 Meal | Whole wheat muffinWhole wheat muffinWhole whea... | NaN | NaN | 1300.0 | 54.0 | 8.0 | 12.0 | 8.0 | 740.0 | 152.0 | 34.0 | 62.0 | 18.0 | 2302.0 | Weekend egg muffin breakfast | . 1 Meal | SalmonBroccoliSalt | NaN | NaN | 330.0 | 9.0 | 1.5 | 3.0 | 1.5 | 95.0 | 8.0 | 4.0 | 47.0 | 4.0 | 1210.0 | Seafood dinner | . 2 Meal | Whole wheat pastaCarrotsMarinara sauce | NaN | NaN | 450.0 | 4.0 | 0.0 | 0.0 | 0.0 | 0.0 | 96.0 | 14.0 | 18.0 | 18.0 | 343.0 | Pasta lunch | . 3 Meal | Mangos | NaN | NaN | 75.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 20.0 | 19.0 | 1.0 | 1.0 | 1.0 | Mango dessert | . 4 Monday | Whole wheat cerealUnsweet soymilkBananaStrawbe... | NaN | NaN | 455.0 | 5.0 | 0.5 | 1.0 | 2.5 | 0.0 | 92.0 | 22.0 | 15.0 | 13.0 | 79.0 | Standard breakfast | . 5 Monday | Whole wheat breadPeanut butterCarrotsStrawberr... | NaN | NaN | 515.0 | 20.0 | 3.5 | 0.0 | 2.0 | 0.0 | 75.0 | 29.0 | 16.0 | 11.0 | 558.0 | Standard lunch | . 6 Monday | Boneless skinless chicken breastSpinachTomatoe... | NaN | NaN | 365.0 | 17.5 | 3.0 | 1.0 | 0.5 | 100.0 | 8.0 | 0.0 | 31.0 | 3.0 | 576.0 | Standard dinner | . 7 Monday | Mangos | NaN | NaN | 75.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 20.0 | 19.0 | 1.0 | 1.0 | 1.0 | Mango dessert | . 8 Monday | Vanilla soymilk | NaN | NaN | 100.0 | 3.5 | 0.5 | 0.5 | 2.0 | 0.0 | 11.0 | 9.0 | 6.0 | 1.0 | 85.0 | Vanilla soymilk drink | . 9 Tuesday | Whole wheat cerealUnsweet soymilkBananaStrawbe... | NaN | NaN | 455.0 | 5.0 | 0.5 | 1.0 | 2.5 | 0.0 | 92.0 | 22.0 | 15.0 | 13.0 | 79.0 | Standard breakfast | . 10 Tuesday | Whole wheat breadPeanut butterCarrotsStrawberr... | NaN | NaN | 515.0 | 20.0 | 3.5 | 0.0 | 2.0 | 0.0 | 75.0 | 29.0 | 16.0 | 11.0 | 558.0 | Standard lunch | . 11 Tuesday | Boneless skinless chicken breastSpinachTomatoe... | NaN | NaN | 365.0 | 17.5 | 3.0 | 1.0 | 0.5 | 100.0 | 8.0 | 0.0 | 31.0 | 3.0 | 576.0 | Standard dinner | . 12 Tuesday | Mangos | NaN | NaN | 75.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 20.0 | 19.0 | 1.0 | 1.0 | 1.0 | Mango dessert | . 13 Tuesday | Vanilla soymilk | NaN | NaN | 100.0 | 3.5 | 0.5 | 0.5 | 2.0 | 0.0 | 11.0 | 9.0 | 6.0 | 1.0 | 85.0 | Vanilla soymilk drink | . 14 Wednesday | Whole wheat cerealUnsweet soymilkBananaStrawbe... | NaN | NaN | 455.0 | 5.0 | 0.5 | 1.0 | 2.5 | 0.0 | 92.0 | 22.0 | 15.0 | 13.0 | 79.0 | Standard breakfast | . 15 Wednesday | Whole wheat breadPeanut butterCarrotsStrawberr... | NaN | NaN | 515.0 | 20.0 | 3.5 | 0.0 | 2.0 | 0.0 | 75.0 | 29.0 | 16.0 | 11.0 | 558.0 | Standard lunch | . 16 Wednesday | Boneless skinless chicken breastSpinachTomatoe... | NaN | NaN | 365.0 | 17.5 | 3.0 | 1.0 | 0.5 | 100.0 | 8.0 | 0.0 | 31.0 | 3.0 | 576.0 | Standard dinner | . 17 Wednesday | Mangos | NaN | NaN | 75.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 20.0 | 19.0 | 1.0 | 1.0 | 1.0 | Mango dessert | . 18 Wednesday | Vanilla soymilk | NaN | NaN | 100.0 | 3.5 | 0.5 | 0.5 | 2.0 | 0.0 | 11.0 | 9.0 | 6.0 | 1.0 | 85.0 | Vanilla soymilk drink | . 19 Thursday | Whole wheat breadPeanut butterCarrotsStrawberr... | NaN | NaN | 515.0 | 20.0 | 3.5 | 0.0 | 2.0 | 0.0 | 75.0 | 29.0 | 16.0 | 11.0 | 558.0 | Standard lunch | . 20 Thursday | Whole wheat muffinEggsBananaAmerican cheese | NaN | NaN | 470.0 | 20.5 | 4.0 | 6.0 | 3.5 | 370.0 | 53.0 | 16.0 | 22.0 | 6.0 | 671.0 | Weekday egg muffin breakfast | . 21 Thursday | Brown ricePinto beansBroccoli | NaN | NaN | 980.0 | 4.5 | 0.0 | 0.0 | 0.0 | 0.0 | 192.0 | 6.0 | 41.0 | 38.0 | 58.0 | Rice and pinto beans dinner | . 22 Thursday | Mangos | NaN | NaN | 75.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 20.0 | 19.0 | 1.0 | 1.0 | 1.0 | Mango dessert | . 23 Thursday | Vanilla soymilk | NaN | NaN | 100.0 | 3.5 | 0.5 | 0.5 | 2.0 | 0.0 | 11.0 | 9.0 | 6.0 | 1.0 | 85.0 | Vanilla soymilk drink | . 24 Friday | Whole wheat breadPeanut butterCarrotsStrawberr... | NaN | NaN | 515.0 | 20.0 | 3.5 | 0.0 | 2.0 | 0.0 | 75.0 | 29.0 | 16.0 | 11.0 | 558.0 | Standard lunch | . 25 Friday | Whole wheat muffinEggsBananaAmerican cheese | NaN | NaN | 470.0 | 20.5 | 4.0 | 6.0 | 3.5 | 370.0 | 53.0 | 16.0 | 22.0 | 6.0 | 671.0 | Weekday egg muffin breakfast | . 26 Friday | Brown ricePinto beansBroccoli | NaN | NaN | 980.0 | 4.5 | 0.0 | 0.0 | 0.0 | 0.0 | 192.0 | 6.0 | 41.0 | 38.0 | 58.0 | Rice and pinto beans dinner | . 27 Friday | Mangos | NaN | NaN | 75.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 20.0 | 19.0 | 1.0 | 1.0 | 1.0 | Mango dessert | . 28 Friday | Vanilla soymilk | NaN | NaN | 100.0 | 3.5 | 0.5 | 0.5 | 2.0 | 0.0 | 11.0 | 9.0 | 6.0 | 1.0 | 85.0 | Vanilla soymilk drink | . 29 Saturday | Whole wheat muffinWhole wheat muffinEggsBanana... | NaN | NaN | 650.0 | 27.0 | 4.0 | 6.0 | 4.0 | 370.0 | 76.0 | 17.0 | 31.0 | 9.0 | 1151.0 | Weekend egg muffin breakfast | . 30 Saturday | SalmonBroccoliSalt | NaN | NaN | 330.0 | 9.0 | 1.5 | 3.0 | 1.5 | 95.0 | 8.0 | 4.0 | 47.0 | 4.0 | 1210.0 | Seafood dinner | . 31 Saturday | Whole wheat pastaCarrotsMarinara sauce | NaN | NaN | 450.0 | 4.0 | 0.0 | 0.0 | 0.0 | 0.0 | 96.0 | 14.0 | 18.0 | 18.0 | 343.0 | Pasta lunch | . 32 Saturday | Mangos | NaN | NaN | 75.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 20.0 | 19.0 | 1.0 | 1.0 | 1.0 | Mango dessert | . 33 Sunday | Whole wheat muffinWhole wheat muffinWhole whea... | NaN | NaN | 1300.0 | 54.0 | 8.0 | 12.0 | 8.0 | 740.0 | 152.0 | 34.0 | 62.0 | 18.0 | 2302.0 | Weekend egg muffin breakfast | . 34 Sunday | SalmonBroccoliSalt | NaN | NaN | 330.0 | 9.0 | 1.5 | 3.0 | 1.5 | 95.0 | 8.0 | 4.0 | 47.0 | 4.0 | 1210.0 | Seafood dinner | . 35 Sunday | Whole wheat pastaCarrotsMarinara sauce | NaN | NaN | 450.0 | 4.0 | 0.0 | 0.0 | 0.0 | 0.0 | 96.0 | 14.0 | 18.0 | 18.0 | 343.0 | Pasta lunch | . 36 Sunday | Mangos | NaN | NaN | 75.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 20.0 | 19.0 | 1.0 | 1.0 | 1.0 | Mango dessert | . Generating The Summary . using the grouping function in Pandas, we can genereate a summary dataframe . df_grouped = df_summary[4:].groupby([&#39;Day&#39;]).sum().reindex(days) df_grouped . Calories Total Fat Saturated Fat Monounsaturated Fat Polyunsaturated Fat Cholesterol Carbs Sugar Protein Fiber Sodium . Day . Monday 1510.0 | 46.0 | 7.5 | 2.5 | 7.0 | 100.0 | 206.0 | 79.0 | 69.0 | 29.0 | 1299.0 | . Tuesday 1510.0 | 46.0 | 7.5 | 2.5 | 7.0 | 100.0 | 206.0 | 79.0 | 69.0 | 29.0 | 1299.0 | . Wednesday 1510.0 | 46.0 | 7.5 | 2.5 | 7.0 | 100.0 | 206.0 | 79.0 | 69.0 | 29.0 | 1299.0 | . Thursday 2140.0 | 48.5 | 8.0 | 6.5 | 7.5 | 370.0 | 351.0 | 79.0 | 86.0 | 57.0 | 1373.0 | . Friday 2140.0 | 48.5 | 8.0 | 6.5 | 7.5 | 370.0 | 351.0 | 79.0 | 86.0 | 57.0 | 1373.0 | . Saturday 1505.0 | 40.0 | 5.5 | 9.0 | 5.5 | 465.0 | 200.0 | 54.0 | 97.0 | 32.0 | 2705.0 | . Sunday 2155.0 | 67.0 | 9.5 | 15.0 | 9.5 | 835.0 | 276.0 | 71.0 | 128.0 | 41.0 | 3856.0 | . Tests Cells . The following cells were used to test solutions I tried when generating the for loop. . It was easier to run these on individual cells rather than in the nested for loop and print statements . df_test = df_days[df_days[&quot;Thursday&quot;] == 1] #df_test[[&quot;Meal&quot;,&quot;Monday&quot;]] df_test print(df_days.columns.values[1:8]) . [&#39;Monday&#39; &#39;Tuesday&#39; &#39;Wednesday&#39; &#39;Thursday&#39; &#39;Friday&#39; &#39;Saturday&#39; &#39;Sunday&#39;] . df_test2 = df_meals[df_meals[&quot;Standard breakfast&quot;] ==1] df_test2 . Meal Standard breakfast Standard lunch Standard dinner Weekday egg muffin breakfast Weekend egg muffin breakfast Seafood dinner Pasta lunch Rice and pinto beans dinner Mango dessert Vanilla soymilk drink . 2 Whole wheat cereal | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12 Unsweet soymilk | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 17 Banana | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | . 18 Strawberries | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . df_test2[[&quot;Meal&quot;]] . Meal . 2 Whole wheat cereal | . 12 Unsweet soymilk | . 17 Banana | . 18 Strawberries | . ingredient_list = [] ingredient_list = df_test2[[&quot;Meal&quot;]].reset_index().drop(columns = &quot;index&quot;) ingredient_list . Meal . 0 Whole wheat cereal | . 1 Unsweet soymilk | . 2 Banana | . 3 Strawberries | . len(ingredient_list) . 4 . df_test3 = df_ingredients[df_ingredients[&quot;Item&quot;] == ingredient_list[&quot;Meal&quot;][0]] df_test3 . Item Brand and style Quantity Calories Total Fat Saturated Fat Monounsaturated Fat Polyunsaturated Fat Cholesterol Carbs Sugar Protein Fiber Sodium . 2 Whole wheat cereal | 365 Everyday Value, wheat squares | 60g | 220 | 1.0 | 0.0 | 0.0 | 0.0 | 0 | 47 | 0 | 7 | 5 | 5 | . df_test4 = df_ingredients[df_ingredients[&quot;Item&quot;] == ingredient_list[&quot;Meal&quot;][1]] df_test4 . Item Brand and style Quantity Calories Total Fat Saturated Fat Monounsaturated Fat Polyunsaturated Fat Cholesterol Carbs Sugar Protein Fiber Sodium . 12 Unsweet soymilk | Silk, unsweet vanilla | 1 cup | 80 | 4.0 | 0.5 | 1.0 | 2.5 | 0 | 4 | 1 | 7 | 2 | 70 | . df_test5 = df_test3.append(df_test4) df_test5 . Item Brand and style Quantity Calories Total Fat Saturated Fat Monounsaturated Fat Polyunsaturated Fat Cholesterol Carbs Sugar Protein Fiber Sodium . 2 Whole wheat cereal | 365 Everyday Value, wheat squares | 60g | 220 | 1.0 | 0.0 | 0.0 | 0.0 | 0 | 47 | 0 | 7 | 5 | 5 | . 12 Unsweet soymilk | Silk, unsweet vanilla | 1 cup | 80 | 4.0 | 0.5 | 1.0 | 2.5 | 0 | 4 | 1 | 7 | 2 | 70 | . df_test5.loc[&#39;Total&#39;,:]= df_test5.sum(axis=0) . df_test5 . Item Brand and style Quantity Calories Total Fat Saturated Fat Monounsaturated Fat Polyunsaturated Fat Cholesterol Carbs Sugar Protein Fiber Sodium . 2 Whole wheat cereal | 365 Everyday Value, wheat squares | 60g | 220.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 47.0 | 0.0 | 7.0 | 5.0 | 5.0 | . 12 Unsweet soymilk | Silk, unsweet vanilla | 1 cup | 80.0 | 4.0 | 0.5 | 1.0 | 2.5 | 0.0 | 4.0 | 1.0 | 7.0 | 2.0 | 70.0 | . Total Whole wheat cerealUnsweet soymilkWhole wheat c... | 365 Everyday Value, wheat squaresSilk, unsweet... | 60g1 cup60g1 cup | 600.0 | 10.0 | 1.0 | 2.0 | 5.0 | 0.0 | 102.0 | 2.0 | 28.0 | 14.0 | 150.0 | . df_test5 . Item Brand and style Quantity Calories Total Fat Saturated Fat Monounsaturated Fat Polyunsaturated Fat Cholesterol Carbs Sugar Protein Fiber Sodium . 2 Whole wheat cereal | 365 Everyday Value, wheat squares | 60g | 220.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 47.0 | 0.0 | 7.0 | 5.0 | 5.0 | . 12 Unsweet soymilk | Silk, unsweet vanilla | 1 cup | 80.0 | 4.0 | 0.5 | 1.0 | 2.5 | 0.0 | 4.0 | 1.0 | 7.0 | 2.0 | 70.0 | . Total Whole wheat cerealUnsweet soymilkWhole wheat c... | 365 Everyday Value, wheat squaresSilk, unsweet... | 60g1 cup60g1 cup | 600.0 | 10.0 | 1.0 | 2.0 | 5.0 | 0.0 | 102.0 | 2.0 | 28.0 | 14.0 | 150.0 | . df_meal_day = df_meals[df_meals[&quot;Weekend egg muffin breakfast&quot;] &gt;= 1] display(df_meal_day) display(df_meals) . Meal Standard breakfast Standard lunch Standard dinner Weekday egg muffin breakfast Weekend egg muffin breakfast Seafood dinner Pasta lunch Rice and pinto beans dinner Mango dessert Vanilla soymilk drink . 3 Whole wheat muffin | 0 | 0 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | . 10 Eggs | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | . 17 Banana | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | . 22 American cheese | 0 | 0 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | . Meal Standard breakfast Standard lunch Standard dinner Weekday egg muffin breakfast Weekend egg muffin breakfast Seafood dinner Pasta lunch Rice and pinto beans dinner Mango dessert Vanilla soymilk drink . 0 Brown rice | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 Whole wheat bread | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 Whole wheat cereal | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 Whole wheat muffin | 0 | 0 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | . 4 Whole wheat pasta | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 Boneless skinless chicken breast | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 Salmon | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 7 Pinto beans | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 8 Black beans | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 Peanut butter | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 10 Eggs | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | . 11 Vanilla soymilk | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 12 Unsweet soymilk | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 13 Spinach | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 14 Broccoli | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 15 Carrots | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 16 Tomatoes | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 17 Banana | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | . 18 Strawberries | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 19 Mangos | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 20 Strawberry jam | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 21 Marinara sauce | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | . 22 American cheese | 0 | 0 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | . 23 Olive oil | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 24 Salt | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . food_list = df_meal_day[[&quot;Meal&quot;, &quot;Standard breakfast&quot;]].reset_index().drop(columns = &quot;index&quot;) food_list . Meal Standard breakfast . 0 Mangos | 0 | . df_day[[&quot;Meal&quot;, &quot;Monday&quot;]] . Meal Monday . 4 Weekend egg muffin breakfast | 0 | . 5 Seafood dinner | 0 | . 6 Pasta lunch | 0 | . 8 Mango dessert | 1 | . meal_list . Meal Sunday . 0 Weekend egg muffin breakfast | 2 | . 1 Seafood dinner | 1 | . 2 Pasta lunch | 1 | . 3 Mango dessert | 1 | . df_test6 = meal_list[meal_list[&quot;Sunday&quot;]&gt;1][&quot;Sunday&quot;] df_test6 . 0 2 Name: Sunday, dtype: int64 . df_test7 = meal_list[&quot;Meal&quot;][meal_list[&quot;Sunday&quot;]==2] df_test7 . 0 Weekend egg muffin breakfast Name: Meal, dtype: object . cats = [ &#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;, &#39;Saturday&#39;, &#39;Sunday&#39;] df_weekday = df_summary[4:].groupby([&#39;Day&#39;]).sum().reindex(cats) df_weekday . Calories Total Fat Saturated Fat Monounsaturated Fat Polyunsaturated Fat Cholesterol Carbs Sugar Protein Fiber Sodium . Day . Monday 1510.0 | 46.0 | 7.5 | 2.5 | 7.0 | 100.0 | 206.0 | 79.0 | 69.0 | 29.0 | 1299.0 | . Tuesday 1510.0 | 46.0 | 7.5 | 2.5 | 7.0 | 100.0 | 206.0 | 79.0 | 69.0 | 29.0 | 1299.0 | . Wednesday 1510.0 | 46.0 | 7.5 | 2.5 | 7.0 | 100.0 | 206.0 | 79.0 | 69.0 | 29.0 | 1299.0 | . Thursday 2140.0 | 48.5 | 8.0 | 6.5 | 7.5 | 370.0 | 351.0 | 79.0 | 86.0 | 57.0 | 1373.0 | . Friday 2140.0 | 48.5 | 8.0 | 6.5 | 7.5 | 370.0 | 351.0 | 79.0 | 86.0 | 57.0 | 1373.0 | . Saturday 1505.0 | 40.0 | 5.5 | 9.0 | 5.5 | 465.0 | 200.0 | 54.0 | 97.0 | 32.0 | 2705.0 | . Sunday 2155.0 | 67.0 | 9.5 | 15.0 | 9.5 | 835.0 | 276.0 | 71.0 | 128.0 | 41.0 | 3856.0 | .",
            "url": "https://progressedd.github.io/blog/2020/05/18/Nutrition-For-Week-Analysis.html",
            "relUrl": "/2020/05/18/Nutrition-For-Week-Analysis.html",
            "date": " • May 18, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Verifying a 3 Meter Walk",
            "content": "Table of Contents . 1&nbsp;&nbsp;Verifying a 3 Meter Walk | 2&nbsp;&nbsp;Introduction | 3&nbsp;&nbsp;Import libraries for processing | 4&nbsp;&nbsp;General Function That Calculates and Outputs Differences | 5&nbsp;&nbsp;Functions For Calculating Differences | 6&nbsp;&nbsp;The Actual Function in use6.0.1&nbsp;&nbsp;IF YOU GET A KeyError, THAT MEANS THE MARKER YOU INPUTTED DOES NOT EXIST ON IN THE TSV COLUMNS | . | . | 7&nbsp;&nbsp;Sample Data Tests | 8&nbsp;&nbsp;Test cases for the function | 9&nbsp;&nbsp;Testing ways to import the data9.1&nbsp;&nbsp;Checking The Outputs of filenames and dataframes | . | . Introduction . In my research with the Neuroscience of Dance in Health and Disability Laboratory, I was tasked with verifying that the recorded section of motion capture data of the participant had truely traveled 3 meters. We chose 3 meters as it is the best sampled portion of the walk for analysis and was consistent with past studies. I wrote this in hopes that this would be used in future studies so that a researcher would not need to calculate by hand when verifying and picking a section of the recording. . # Have the notebook full screen from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;&quot;)) . Import libraries for processing . pandas for the dataframes | random for simulation | glob for importing files | . import pandas as pd import random import glob . General Function That Calculates and Outputs Differences . In this function, all files in with the tsv extension in the same directory as this notebook file are loaded into a list called filenames, after being loaded into the list, the first for loop iterates through the filenames list and makes a dataframes list with all the tsv files converted into Pandas DataFrames. . The Function takes an input called marker, which is used to indicate which column we are interested in. . The we use the second loop to iterate through the dataframes list. In each iteration, the difference between the first and last indicies of right heel is calculated and stored in the diff variable. Next we initialize a dictionary d that has the current filename (filenames[i]) and the calculated difference (diff). At the end of the iteration, the dictionary entry will be appended to a list called data. . At the end of the function , the DataFrame df_all is returned giving a DataFrame with the file name and calculated displacement for each row . def walkDisplacement(marker): filenames = glob.glob(&quot;*.tsv&quot;) dataframes = [] data = [] allowedMarkers = [] # going to have a if statement to check if marker variable exists, otherwise python will just yell for f in filenames: dataframes.append(pd.read_csv(f,delimiter=&#39; t&#39;,encoding=&#39;utf-8&#39;)) for i in range(len(dataframes)): diff = metercalc(dataframes[i],marker) d = {&quot;Filename&quot;: str(filenames[i]),&quot;Difference (meters)&quot; : diff} data.append(d) df_all = pd.DataFrame(data) return df_all . Functions For Calculating Differences . This is the function calculates the difference of the first and last rows of a dataframe. . The inputs are a dataframe and a string for the column we are interested in. I called it marker because in our case we are dealing with marker values . def difference(df,marker): first = df[marker].iloc[0] last = df[marker].iloc[-1] diff = last - first return diff . This function calculates the difference of the first and last rows of a the walk. . The function is an adaptation of the logic Andrea wrote for calculating the amount of distance walked based on the position data . I added a second conditional for cases where the difference is negative . Example for TDP MS W MRI 001 10MWT2014223mmatlab.qtm . Get the starting position xi (=2717.08) of the right heel | Calculate the difference d by subtracting 3000mm from xi (because 3m=3000mm) d=2717.08-3000=-282.92mm | Get the finish position at the end of the crop section of the right heel xf (=-321.77) | If xf doesn’t equal d then check the exact difference. For example d=-282.92mm and xf=-321.77 get the difference between them –38.85. | The exact difference will now be 3000+38.85=3038.85mm which equals 3.038m | def metercalc(df,marker): xi = df[marker].iloc[0] d = xi - 3000 xf = df[marker].iloc[-1] if (xf != d): # checks if xi is equal to xf diff = xf - d # if not then it will take the difference between the two values if(diff &lt; 0): # checks if the difference is negative exact = 3000 - (diff) return exact/1000 # will return flipped difference if negative return diff/1000 # will return the positive difference . The Actual Function in use . To use walkDisplacement, place all raw tsv files into the same directory as this notebook, next, input the column name of the marker you are interested in. The following example will use the right heel . walkDisplacement(&quot;right heel&quot;) . Filename Difference (meters) . 0 test1.tsv | 3.859 | . 1 test2.tsv | 2.314 | . 2 test3.tsv | 3.073 | . 3 test4.tsv | 0.760 | . IF YOU GET A KeyError, THAT MEANS THE MARKER YOU INPUTTED DOES NOT EXIST ON IN THE TSV COLUMNS . Sample Data Tests . I made a sample data set to test the initial difference function . numbers =[1,2,3,4,5,6,7,8,9] df_n = pd.DataFrame({&quot;right heel&quot;: numbers}) df_n . right heel . 0 1 | . 1 2 | . 2 3 | . 3 4 | . 4 5 | . 5 6 | . 6 7 | . 7 8 | . 8 9 | . This was a simulated data set I generated to test the metercalc function for values over 0 . data = [] for i in range(10): lower = random.randint(1000,6000) d = {&quot;right heel&quot;: lower} data.append(d) sim = pd.DataFrame(data) sim . right heel . 0 5778 | . 1 5214 | . 2 3172 | . 3 5992 | . 4 5218 | . 5 4846 | . 6 5449 | . 7 1962 | . 8 1267 | . 9 1334 | . Andrea&#39;s example . ae = [2717.08,-321.77] df_ae = pd.DataFrame({&quot;right heel&quot; : ae}) df_ae . right heel . 0 2717.08 | . 1 -321.77 | . These lines of code were to test out the .iloc function, I tried it here first before implementing it to the difference function . first = df_n[&quot;right heel&quot;].iloc[0] last = df_n[&quot;right heel&quot;].iloc[-1] diff = last - first diff . 8 . Test cases for the function . Below are tests I ran to check the accuracy of the functions . I wrote this test case initially to see if my difference function outputted the correct value . the intial value was 1 and final value was 9, the difference that should be outputted is 8 = (9-1) . right_diff = difference(df_n, &quot;right heel&quot;) print(right_diff) . 8 . I wrote this test case to check the metercalc function was outputting the correct value given random data that is in the ranges of 1000-6000 . actual_test = metercalc(sim, &quot;right heel&quot;) actual_test . 2.698 . Testing ways to import the data . I wanted to make a dataframe that outputted the values, this is still under construction. . Right now it is a for loop that takes a list of dataframes, runs the metercalc function on each entry and outputs a dataframe of the calculations. this section is my tests I did to make the general function . If I am given more time, I will turn this into a function that can take all the tsv files in a directory, convert them into dataframes, and make a list of all the dataframes that will be processed . data = [] for i in range(len(dataframes)): diff = metercalc(dataframes[i], &quot;right heel&quot;) d = {&quot;filename&quot;: str(filenames[i]), &quot;Difference (meters)&quot;: diff} data.append(d) df_all = pd.DataFrame(data) df_all . filename Difference (meters) . 0 test1.tsv | 3.859 | . 1 test2.tsv | 2.314 | . 2 test3.tsv | 3.073 | . 3 test4.tsv | 0.760 | . I did some research and found a page that showed how to import files and load it into a data frames . I asked for some help from sigpwny and rats_irl helped me write some code based on the link I showed to him . filenames is a variable that stores a list of the tsv files of a directory . dataframes is a list . the for loop iterates through filenames and for each entry, and converts the tsv files into dataframes . with this I can make a general function . filenames = glob.glob(&quot;*.tsv&quot;) dataframes = [] for f in filenames: dataframes.append(pd.read_csv(f,delimiter=&#39; t&#39;,encoding=&#39;utf-8&#39;)) . Checking The Outputs of filenames and dataframes . print(&quot;name of the file = &quot; + str(filenames[0])) display(dataframes[0]) . name of the file = test1.tsv . Unnamed: 0 right heel . 0 0 | 3516 | . 1 1 | 2689 | . 2 2 | 5181 | . 3 3 | 1408 | . 4 4 | 2545 | . 5 5 | 2229 | . 6 6 | 5266 | . 7 7 | 1686 | . 8 8 | 1558 | . 9 9 | 4375 | . for i in range(len(dataframes)): display(dataframes[i]) . Unnamed: 0 right heel . 0 0 | 3516 | . 1 1 | 2689 | . 2 2 | 5181 | . 3 3 | 1408 | . 4 4 | 2545 | . 5 5 | 2229 | . 6 6 | 5266 | . 7 7 | 1686 | . 8 8 | 1558 | . 9 9 | 4375 | . Unnamed: 0 right heel . 0 0 | 3030 | . 1 1 | 5689 | . 2 2 | 1525 | . 3 3 | 4448 | . 4 4 | 5904 | . 5 5 | 3180 | . 6 6 | 1661 | . 7 7 | 2981 | . 8 8 | 5494 | . 9 9 | 2344 | . Unnamed: 0 right heel . 0 0 | 5448 | . 1 1 | 4231 | . 2 2 | 5242 | . 3 3 | 1325 | . 4 4 | 3561 | . 5 5 | 2519 | . 6 6 | 2443 | . 7 7 | 4251 | . 8 8 | 3286 | . 9 9 | 2375 | . Unnamed: 0 right heel . 0 0 | 5273 | . 1 1 | 2096 | . 2 2 | 4345 | . 3 3 | 2171 | . 4 4 | 2170 | . 5 5 | 1516 | . 6 6 | 4599 | . 7 7 | 1919 | . 8 8 | 5891 | . 9 9 | 3033 | .",
            "url": "https://progressedd.github.io/blog/2020/04/24/calculating-3m-walk.html",
            "relUrl": "/2020/04/24/calculating-3m-walk.html",
            "date": " • Apr 24, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Nutrient Density and Value Data Visualization",
            "content": "Table of Contents . 1&nbsp;&nbsp;Nutrient Density and Value Data Visualization | 2&nbsp;&nbsp;About | 3&nbsp;&nbsp;Importing libraries | 4&nbsp;&nbsp;Raw Data | 5&nbsp;&nbsp;Largest values | 6&nbsp;&nbsp;Plotting Function6.1&nbsp;&nbsp;Arguments | 6.2&nbsp;&nbsp;What Occurs During a Function Call | . | 7&nbsp;&nbsp;Charts7.1&nbsp;&nbsp;All Energy Values Comparison | 7.2&nbsp;&nbsp;All Protein Values Comparison | 7.3&nbsp;&nbsp;All Fat Values Comparison | 7.4&nbsp;&nbsp;All Saturated Fat Values Comparison | 7.5&nbsp;&nbsp;All Cholesterol Values Comparison | 7.6&nbsp;&nbsp;All Vitamn Values Comparison | 7.7&nbsp;&nbsp;All Sodium Values Comparison | 7.8&nbsp;&nbsp;All Phosphorus Values Comparison | 7.9&nbsp;&nbsp;All Iron Values Comparison | 7.10&nbsp;&nbsp;All Zinc Values Comparison | 7.11&nbsp;&nbsp;All Retail Cost Comparison | . | 8&nbsp;&nbsp;Testing8.1&nbsp;&nbsp;Initializing Test Variables | 8.2&nbsp;&nbsp;Testing if Saturated Fat Exists in ref4 Dictonary | 8.3&nbsp;&nbsp;Testing Dictionary Outputs In Relation to Variables | 8.4&nbsp;&nbsp;Testing Solution To Backslashes Causing Issues in Python | . | 9&nbsp;&nbsp;Previous versions of functions9.1&nbsp;&nbsp;Energy Values Plotting Function | 9.2&nbsp;&nbsp;Protein Values Plotting Function | . | . About . This is a project I made to analyze the amount of protein the various animal and vegetable products people can consume. . In this post, I learned the basics of using matplotlib. A future post might be refining this post . The data was imported from Bohrer, B. M. (2017). Review: Nutrient density and nutritional value of meat products and non-meat foods high in protein. Trends in Food Science &amp; Technology, 65, 103-112. doi:10.1016/j.tifs.2017.04.016 . . Importing libraries . The categories variables map to the text from the table. I made these variables to test the function . import pandas as pd import matplotlib as plt import matplotlib.pyplot as plt; plt.rcdefaults() import matplotlib.pyplot as plt import numpy as np df1 = pd.read_csv(&quot;Nutrition.csv&quot;, encoding= &#39;unicode_escape&#39;) df2 = pd.read_csv(&quot;nutrition-cost.csv&quot;, encoding= &#39;unicode_escape&#39;) # categories meat = &quot;Meat, raw/unprepared unless noted otherwise&quot; fish = &quot;Fish, raw/unprepared&quot; non_meat = &quot;Non-meat, raw/unprepared&quot; from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;&quot;)) . Raw Data . Once we have loaded the csv files, I will take a look to see that the values are correct and check if there are any noticeable trends . df1 . Category Product Energy value (kcal) Protein (g) Fat (g) Saturated fat (g) Cholesterol (mg) Vitamin B12 (mcg) Na (mg) P (mg) Fe (mg) Zn (mg) . 0 Meat, raw/unprepared unless noted otherwise | Beef ribeye (longissimus thoracis) | 166 | 17.51 | 22.07 | 9.000 | 68 | 3.11 | 56.00 | 168 | 1.87 | 3.85 | . 1 Meat, raw/unprepared unless noted otherwise | Beef strip loin (longissimus lumborum) | 228 | 20.61 | 15.49 | 6.251 | 81 | 1.06 | 52.00 | 186 | 1.47 | 3.53 | . 2 Meat, raw/unprepared unless noted otherwise | Beef tenderloin (psoas major) | 247 | 19.61 | 18.16 | 8.410 | 85 | 1.01 | 50.00 | 180 | 1.42 | 2.9 | . 3 Meat, raw/unprepared unless noted otherwise | Beef top sirloin (gluteus mediuset al.) | 201 | 20.30 | 12.71 | 5.127 | 75 | 1.05 | 52.00 | 187 | 1.48 | 3.55 | . 4 Meat, raw/unprepared unless noted otherwise | Beef tri-tip (m. tensor fasciae latae) | 142 | 21.26 | 5.63 | 1.729 | 61 | 1.04 | 54.00 | 198 | 1.54 | 3.85 | . 5 Meat, raw/unprepared unless noted otherwise | Beef eye of round (semitendinosus) | 124 | 23.27 | 3.44 | 1.276 | 60 | 1.83 | 53.00 | 221 | 1.45 | 3.4 | . 6 Meat, raw/unprepared unless noted otherwise | Beef brisket (deep pectoral et al.) | 132 | 21.47 | 5.11 | 1.844 | 67 | 1.81 | 83.00 | 222 | 2.06 | 5.21 | . 7 Meat, raw/unprepared unless noted otherwise | Beef flank (rectus abdominis) | 155 | 21.22 | 7.17 | 2.978 | 65 | 1.09 | 54.00 | 195 | 1.55 | 3.7 | . 8 Meat, raw/unprepared unless noted otherwise | Beef, ground 80% lean, 20% fat | 254 | 17.17 | 20.00 | 7.581 | 71 | 2.14 | 66.00 | 158 | 1.94 | 4.18 | . 9 Meat, raw/unprepared unless noted otherwise | Beef, ground 90% lean, 10% fat | 176 | 20.00 | 10.00 | 3.927 | 65 | 2.21 | 66.00 | 184 | 2.24 | 4.79 | . 10 Meat, raw/unprepared unless noted otherwise | Beef, ground 93% lean, 7% fat | 152 | 20.85 | 7.00 | 2.878 | 63 | 2.23 | 66.00 | 192 | 2.33 | 4.97 | . 11 Meat, raw/unprepared unless noted otherwise | Beef, ground 97% lean, 3% fat | 121 | 21.98 | 3.00 | 1.480 | 60 | 2.26 | 66.00 | 203 | 2.44 | 5.21 | . 12 Meat, raw/unprepared unless noted otherwise | Pork loin (longissimus lumborum) | 198 | 19.74 | 12.58 | 4.360 | 63 | 0.53 | 50.00 | 197 | 0.79 | 1.74 | . 13 Meat, raw/unprepared unless noted otherwise | Pork ham (biceps femoris et al.) | 245 | 17.43 | 18.87 | 6.540 | 73 | 0.63 | 47.00 | 199 | 0.85 | 1.93 | . 14 Meat, raw/unprepared unless noted otherwise | Pork, ground 84% lean, 16% fat | 218 | 17.99 | 16.00 | 4.930 | 68 | 0.73 | 68.00 | 161 | 0.88 | 1.91 | . 15 Meat, raw/unprepared unless noted otherwise | Pork, ground 96% lean, 4% fat | 121 | 21.10 | 4.00 | 1.420 | 59 | 0.64 | 67.00 | 190 | 0.86 | 1.93 | . 16 Meat, raw/unprepared unless noted otherwise | Lamb loin (longissimus lumborum) | 310 | 16.32 | 26.63 | 11.760 | 74 | 2.04 | 56.00 | 152 | 1.61 | 2.53 | . 17 Meat, raw/unprepared unless noted otherwise | Lamb leg (biceps femoris et al) | 230 | 17.91 | 17.07 | 7.430 | 69 | 2.5 | 56.00 | 170 | 1.66 | 3.32 | . 18 Meat, raw/unprepared unless noted otherwise | Lamb, ground 85% lean, 15% fat | 255 | 17.14 | 20.71 | 9.926 | 73 |  | 77.00 |  | 1.41 |  | . 19 Meat, raw/unprepared unless noted otherwise | Chicken breast (pectoralis major) | 120 | 22.50 | 2.62 | 0.563 | 73 | 0.21 | 45.00 | 213 | 0.37 | 0.68 | . 20 Meat, raw/unprepared unless noted otherwise | Chicken thigh (iliotibialis et al.) | 221 | 16.52 | 16.61 | 4.524 | 98 | 0.62 | 81.00 | 157 | 0.68 | 1.29 | . 21 Meat, raw/unprepared unless noted otherwise | Turkey breast (pectoralis major) | 157 | 21.89 | 7.02 | 1.910 | 65 | 0.42 | 59.00 | 186 | 1.20 | 1.57 | . 22 Meat, raw/unprepared unless noted otherwise | Turkey thigh (iliotibialis et al.) | 116 | 20.60 | 3.69 | 0.782 | 78 | 2.17 | 75.00 | 177 | 1.42 | 2.95 | . 23 Meat, raw/unprepared unless noted otherwise | Turkey, ground, 93% lean, 15% fat | 150 | 18.73 | 8.34 | 2.170 | 74 | 1.2 | 69.00 | 193 | 1.17 | 2.53 | . 24 Meat, raw/unprepared unless noted otherwise | Pork bacon, cured, unprepared | 417 | 12.62 | 39.69 | 13.296 | 66 | 0.5 | 662.00 | 144 | 0.41 | 1.18 | . 25 Fish, raw/unprepared | Tuna, yellowfin | 109 | 24.40 | 0.49 | 0.172 | 39 | 2.08 | 45.00 | 278 | 0.77 | 0.37 | . 26 Fish, raw/unprepared | Salmon, Atlantic | 142 | 19.84 | 6.34 | 0.981 | 55 | 3.18 | 44.00 | 200 | 0.80 | 0.64 | . 27 Fish, raw/unprepared | Pollock, Atlantic | 92 | 19.44 | 0.98 | 0.135 | 71 | 3.19 | 86.00 | 221 | 0.46 | 0.47 | . 28 Fish, raw/unprepared | Halibut, Atlantic | 91 | 18.56 | 1.33 | 0.292 | 49 | 1.1 | 68.00 | 236 | 0.16 | 0.36 | . 29 Fish, raw/unprepared | Tilapia | 96 | 20.08 | 1.70 | 0.585 | 50 | 1.58 | 52.00 | 170 | 0.56 | 0.33 | . 30 Fish, raw/unprepared | Catfish, wild channel | 95 | 16.38 | 2.82 | 0.722 | 58 | 2.23 | 43.00 | 209 | 0.30 | 0.51 | . 31 Non-meat, raw/unprepared | Chicken eggs, whole | 143 | 12.56 | 9.51 | 3.126 | 372 | 0.89 | 142.00 | 198 | 1.75 | 1.29 | . 32 Non-meat, raw/unprepared | Yogurt, Greek non-fat | 59 | 10.19 | 0.39 | 0.117 | 5 | 0.75 | 36.00 | 135 | 0.07 | 0.52 | . 33 Non-meat, raw/unprepared | Kale | 49 | 4.28 | 0.93 | 0.091 | 0 | 0 | 38.00 | 92 | 1.47 | 0.56 | . 34 Non-meat, raw/unprepared | Lentils, sprouted | 106 | 8.96 | 0.55 | 0.057 | 0 | 0 | 11.00 | 173 | 3.21 | 1.51 | . 35 Non-meat, raw/unprepared | Broccoli, heads | 34 | 2.82 | 0.37 | 0.039 | 0 | 0 | 33.00 | 66 | 0.73 | 0.41 | . 36 Non-meat, raw/unprepared | Green peas | 81 | 5.42 | 0.40 | 0.071 | 0 | 0 | 5.00 | 108 | 1.47 | 1.24 | . 37 Non-meat, raw/unprepared | Spinach | 23 | 2.86 | 0.39 | 0.063 | 0 | 0 | 79.00 | 49 | 2.71 | 0.53 | . 38 Non-meat, raw/unprepared | Black beans, mature | 341 | 21.60 | 1.42 | 0.366 | 0 | 0 | 5.00 | 352 | 5.02 | 3.65 | . 39 Non-meat, raw/unprepared | Pinto beans, mature | 62 | 5.25 | 0.90 | 0.109 | 0 | 0 | 153.00 | 94 | 1.97 | 0.5 | . 40 Non-meat, raw/unprepared | Lima beans, immature | 113 | 6.84 | 0.86 | 0.198 | 0 | 0 | 0.78 | 136 | 3.14 | 0.78 | . 41 Non-meat, raw/unprepared | Kidney beans, mature | 29 | 4.20 | 0.50 | 0.720 | 0 | 0 | 6.00 | 37 | 0.81 | 0.4 | . 42 Non-meat, raw/unprepared | Great northern beans, mature | 339 | 21.86 | 1.14 | 0.356 | 0 | 0 | 14.00 | 447 | 5.47 | 2.31 | . 43 Non-meat, raw/unprepared | Tofu, firm, prepared with CaSO4 and MgCl2 | 78 | 9.04 | 4.17 | 0.793 | 0 | 0 | 12.00 | 121 | 1.61 | 0.83 | . 44 Non-meat, raw/unprepared | Tofu, soft, prepared with CaSO4 and MgCl2 | 61 | 7.17 | 3.69 | 0.533 | 0 | 0 | 8.00 | 92 | 1.11 | 0.64 | . 45 Non-meat, raw/unprepared | Hummus | 166 | 7.90 | 9.60 | 1.437 | 0 | 0 | 379.00 | 176 | 2.44 | 1.83 | . 46 Non-meat, raw/unprepared | Peanuts | 567 | 25.80 | 49.24 | 6.279 | 0 | 0 | 18.00 | 376 | 4.58 | 3.27 | . 47 Non-meat, raw/unprepared | Almonds | 579 | 21.15 | 49.93 | 3.802 | 0 | 0 | 1.00 | 481 | 3.71 | 3.12 | . 48 Non-meat, raw/unprepared | Cashews | 553 | 18.22 | 43.85 | 7.783 | 0 | 0 | 12.00 | 593 | 6.68 | 5.78 | . df2 . Category Product Retail Cost/100 g (US$) Source of estimationb Energy value (kcal/US$) Protein (g/US$) Vitamin B12 (mcg/US$) P (mg/US$) Fe (mg/US$) Zn (mg/US$) . 0 Meat, raw/unprepared unless noted otherwise | Beef ribeye (longissimus thoracis) | 1.81000 | USDA, AMS, 2016 | 91.71 | 9.67 | 1.72 | 92.82 | 1.03 | 2.13 | . 1 Meat, raw/unprepared unless noted otherwise | Beef strip loin (longissimus lumborum) | 1.72620 | USDA, AMS, 2016 | 132.08 | 11.94 | 0.61 | 107.75 | 0.85 | 2.04 | . 2 Meat, raw/unprepared unless noted otherwise | Beef tenderloin (psoas major) | 2.44490 | USDA, AMS, 2016 | 101.03 | 8.02 | 0.41 | 73.62 | 0.58 | 1.19 | . 3 Meat, raw/unprepared unless noted otherwise | Beef top sirloin (gluteus medius et al.) | 1.12440 | USDA, AMS, 2016 | 178.77 | 18.05 | 0.93 | 166.32 | 1.32 | 3.16 | . 4 Meat, raw/unprepared unless noted otherwise | Beef tri-tip (m. tensor fasciae latae) | 1.18610 | USDA, AMS, 2016 | 119.72 | 17.92 | 0.88 | 166.94 | 1.30 | 3.25 | . 5 Meat, raw/unprepared unless noted otherwise | Beef eye of round (semitendinosus) | 1.01410 | USDA, AMS, 2016 | 122.27 | 22.95 | 1.8 | 217.92 | 1.43 | 3.35 | . 6 Meat, raw/unprepared unless noted otherwise | Beef brisket (deep pectoralet al.) | 0.89950 | USDA, AMS, 2016 | 146.75 | 23.87 | 2.01 | 246.81 | 2.29 | 5.79 | . 7 Meat, raw/unprepared unless noted otherwise | Beef flank (rectus abdominis) | 1.69980 | USDA, AMS, 2016 | 91.19 | 12.48 | 0.64 | 114.72 | 0.91 | 2.18 | . 8 Meat, raw/unprepared unless noted otherwise | Beef, ground 80% lean, 20% fat | 0.73190 | USDA, AMS, 2016 | 347.03 | 23.46 | 2.92 | 215.87 | 2.65 | 5.71 | . 9 Meat, raw/unprepared unless noted otherwise | Beef, ground 90% lean, 10% fat | 1.03620 | USDA, AMS, 2016 | 169.86 | 19.30 | 2.13 | 177.58 | 2.16 | 4.62 | . 10 Meat, raw/unprepared unless noted otherwise | Beef, ground 93% lean, 7% fat | 1.03620 | USDA, AMS, 2016 | 146.69 | 20.12 | 2.15 | 185.3 | 2.25 | 4.8 | . 11 Meat, raw/unprepared unless noted otherwise | Beef, ground 97% lean, 3% fat | 1.03620 | USDA, AMS, 2016 | 116.78 | 21.21 | 2.18 | 195.91 | 2.35 | 5.03 | . 12 Meat, raw/unprepared unless noted otherwise | Pork loin (longissimus lumborum) | 0.41230 | USDA, AMS, 2016 | 480.27 | 47.88 | 1.29 | 477.85 | 1.92 | 4.22 | . 13 Meat, raw/unprepared unless noted otherwise | Pork ham (biceps femoriset al.) | 0.70550 | USDA, AMS, 2016 | 347.28 | 24.71 | 0.89 | 282.08 | 1.20 | 2.74 | . 14 Meat, raw/unprepared unless noted otherwise | Pork bacon, cured, unprepared | 4.72890 | USDA, AMS, 2016 | 88.18 | 2.67 | 0.11 | 30.45 | 0.09 | 0.25 | . 15 Meat, raw/unprepared unless noted otherwise | Pork, ground 84% lean, 16% fat | 0.59300 | USDA, AMS, 2016 | 367.60 | 30.34 | 1.23 | 271.48 | 1.48 | 3.22 | . 16 Meat, raw/unprepared unless noted otherwise | Pork, ground 96% lean, 4% fat | 0.59300 | USDA, AMS, 2016 | 204.03 | 35.58 | 1.08 | 320.38 | 1.45 | 3.25 | . 17 Meat, raw/unprepared unless noted otherwise | Lamb loin (longissimus lumborum) | 1.78570 | USDA, AMS, 2016 | 173.60 | 9.14 | 1.14 | 85.12 | 0.90 | 1.42 | . 18 Meat, raw/unprepared unless noted otherwise | Lamb leg (biceps femoris et al) | 1.76150 | USDA, AMS, 2016 | 130.57 | 10.17 | 1.42 | 96.51 | 0.94 | 1.88 | . 19 Meat, raw/unprepared unless noted otherwise | Lamb, ground 85% lean, 15% fat | 1.24120 | USDA, AMS, 2016 | 205.45 | 13.81 |  |  | 1.14 |  | . 20 Meat, raw/unprepared unless noted otherwise | Chicken breast (pectoralis major) | 0.70110 | USDA, AMS, 2016 | 171.17 | 32.09 | 0.3 | 303.82 | 0.53 | 0.97 | . 21 Meat, raw/unprepared unless noted otherwise | Chicken thigh (iliotibialiset al.) | 0.28440 | USDA, AMS, 2016 | 777.09 | 58.09 | 2.18 | 552.05 | 2.39 | 4.54 | . 22 Meat, raw/unprepared unless noted otherwise | Turkey breast (pectoralis major) | 0.59300 | USDA, AMS, 2016 | 264.74 | 36.91 | 0.71 | 313.64 | 2.02 | 2.65 | . 23 Meat, raw/unprepared unless noted otherwise | Turkey thigh (iliotibialiset al.) | 0.66140 | USDA, AMS, 2016 | 175.39 | 31.15 | 3.28 | 267.62 | 2.15 | 4.46 | . 24 Meat, raw/unprepared unless noted otherwise | Turkey, ground, 93% lean, 15% fat | 0.78480 | USDA, AMS, 2016 | 191.12 | 23.86 | 1.53 | 245.91 | 1.49 | 3.22 | . 25 Fish, raw/unprepared | Tuna, yellowfin | 5.01820 | Marketplace assessment | 21.72 | 4.86 | 0.41 | 55.4 | 0.15 | 0.07 | . 26 Fish, raw/unprepared | Salmon, Atlantic | 4.54060 | Marketplace assessment | 31.27 | 4.37 | 0.7 | 44.05 | 0.18 | 0.14 | . 27 Fish, raw/unprepared | Pollock, Atlantic | 0.81430 | Marketplace assessment | 112.98 | 23.87 | 3.92 | 271.4 | 0.56 | 0.58 | . 28 Fish, raw/unprepared | Halibut, Atlantic | 7.30520 | Marketplace assessment | 12.46 | 2.54 | 0.15 | 32.31 | 0.02 | 0.05 | . 29 Fish, raw/unprepared | Tilapia | 2.00220 | Marketplace assessment | 47.95 | 10.03 | 0.79 | 84.91 | 0.28 | 0.16 | . 30 Fish, raw/unprepared | Catfish, wild channel | 2.36250 | Marketplace assessment | 40.21 | 6.93 | 0.94 | 88.47 | 0.13 | 0.22 | . 31 Non-meat, raw/unprepared | Chicken eggs, whole | 0.18330 | USDA, AMS, 2016 | 780.00 | 68.51 | 4.85 | 1080 | 9.55 | 7.04 | . 32 Non-meat, raw/unprepared | Yogurt, Greek non-fat | 0.86420 | USDA, AMS, 2016 | 68.27 | 11.79 | 0.87 | 156.21 | 0.08 | 0.6 | . 33 Non-meat, raw/unprepared | Kale | 0.61950 | USDA, ERS, 2013 | 79.10 | 6.91 | 0 | 148.51 | 2.37 | 0.9 | . 34 Non-meat, raw/unprepared | Lentils, sprouted | 0.30640 | USDA, ERS, 2013 | 345.91 | 29.24 | 0 | 564.54 | 10.48 | 4.93 | . 35 Non-meat, raw/unprepared | Broccoli, heads | 0.36160 | USDA, ERS, 2013 | 94.04 | 7.80 | 0 | 182.54 | 2.02 | 1.13 | . 36 Non-meat, raw/unprepared | Green peas | 0.36380 | USDA, ERS, 2013 | 222.67 | 14.90 | 0 | 296.9 | 4.04 | 3.41 | . 37 Non-meat, raw/unprepared | Spinach | 0.84440 | USDA, ERS, 2013 | 27.24 | 3.39 | 0 | 58.03 | 3.21 | 0.63 | . 38 Non-meat, raw/unprepared | Black beans, mature | 0.32849 | USDA, ERS, 2013 | 1038.09 | 65.76 | 0 | 1071.58 | 15.28 | 11.11 | . 39 Non-meat, raw/unprepared | Pinto beans, mature | 0.26455 | USDA, ERS, 2013 | 234.36 | 19.84 | 0 | 355.31 | 7.45 | 1.89 | . 40 Non-meat, raw/unprepared | Lima beans, immature | 0.38140 | USDA, ERS, 2013 | 296.28 | 17.93 | 0 | 356.58 | 8.23 | 2.05 | . 41 Non-meat, raw/unprepared | Kidney beans, mature | 0.36820 | USDA, ERS, 2013 | 78.77 | 11.41 | 0 | 100.5 | 2.20 | 1.09 | . 42 Non-meat, raw/unprepared | Great northern beans, mature | 0.33730 | USDA, ERS, 2013 | 1005.02 | 64.81 | 0 | 1325.2 | 16.22 | 6.85 | . 43 Non-meat, raw/unprepared | Tofu, firm, prepared with CaSO4 and MgCl2 | 0.97710 | Marketplace assessment | 79.83 | 9.25 | 0 | 123.84 | 1.65 | 0.85 | . 44 Non-meat, raw/unprepared | Tofu, soft, prepared with CaSO4 and MgCl2 | 0.44500 | Marketplace assessment | 137.08 | 16.11 | 0 | 206.74 | 2.49 | 1.44 | . 45 Non-meat, raw/unprepared | Hummus | 0.92500 | Marketplace assessment | 179.46 | 8.54 | 0 | 190.27 | 2.64 | 1.98 | . 46 Non-meat, raw/unprepared | Peanuts | 0.87960 | Marketplace assessment | 644.58 | 29.33 | 0 | 427.45 | 5.21 | 3.72 | . 47 Non-meat, raw/unprepared | Almonds | 1.98200 | Marketplace assessment | 292.14 | 10.67 | 0 | 242.69 | 1.87 | 1.57 | . 48 Non-meat, raw/unprepared | Cashews | 2.20240 | Marketplace assessment | 251.09 | 8.27 | 0 | 269.25 | 3.03 | 2.62 | . Largest values . This Dataframe displays the 10 products that have the most energy value (kcal/US$). I wanted to see which protein product gave you the most Energy for how much you pay. . df2.nlargest(n = 10, columns = &quot;Energy value (kcal/US$)&quot;) . Category Product Retail Cost/100 g (US$) Source of estimationb Energy value (kcal/US$) Protein (g/US$) Vitamin B12 (mcg/US$) P (mg/US$) Fe (mg/US$) Zn (mg/US$) . 38 Non-meat, raw/unprepared | Black beans, mature | 0.32849 | USDA, ERS, 2013 | 1038.09 | 65.76 | 0 | 1071.58 | 15.28 | 11.11 | . 42 Non-meat, raw/unprepared | Great northern beans, mature | 0.33730 | USDA, ERS, 2013 | 1005.02 | 64.81 | 0 | 1325.2 | 16.22 | 6.85 | . 31 Non-meat, raw/unprepared | Chicken eggs, whole | 0.18330 | USDA, AMS, 2016 | 780.00 | 68.51 | 4.85 | 1080 | 9.55 | 7.04 | . 21 Meat, raw/unprepared unless noted otherwise | Chicken thigh (iliotibialiset al.) | 0.28440 | USDA, AMS, 2016 | 777.09 | 58.09 | 2.18 | 552.05 | 2.39 | 4.54 | . 46 Non-meat, raw/unprepared | Peanuts | 0.87960 | Marketplace assessment | 644.58 | 29.33 | 0 | 427.45 | 5.21 | 3.72 | . 12 Meat, raw/unprepared unless noted otherwise | Pork loin (longissimus lumborum) | 0.41230 | USDA, AMS, 2016 | 480.27 | 47.88 | 1.29 | 477.85 | 1.92 | 4.22 | . 15 Meat, raw/unprepared unless noted otherwise | Pork, ground 84% lean, 16% fat | 0.59300 | USDA, AMS, 2016 | 367.60 | 30.34 | 1.23 | 271.48 | 1.48 | 3.22 | . 13 Meat, raw/unprepared unless noted otherwise | Pork ham (biceps femoriset al.) | 0.70550 | USDA, AMS, 2016 | 347.28 | 24.71 | 0.89 | 282.08 | 1.20 | 2.74 | . 8 Meat, raw/unprepared unless noted otherwise | Beef, ground 80% lean, 20% fat | 0.73190 | USDA, AMS, 2016 | 347.03 | 23.46 | 2.92 | 215.87 | 2.65 | 5.71 | . 34 Non-meat, raw/unprepared | Lentils, sprouted | 0.30640 | USDA, ERS, 2013 | 345.91 | 29.24 | 0 | 564.54 | 10.48 | 4.93 | . Plotting Function . These are functions I wrote to plot energy value and protein content. . Arguments . The arguments include a dataframe, and a string variable called category, the variable category helps determine whether we will plot a specific protein product or if it will plot all graphs. . What Occurs During a Function Call . When the function is called, the variables skipCheck, categoryCase, and columnTitleCase are initialized the variables that end in case, convert the lowercase to a title to match the dictionary entry. Next, a dictionary is initialized for all the categories of the protein products. In the next line, we will use a if statement to check which dataframe we are processing, since df2 has the retail cost, we will look for retail cost, if the column exists, we will set columnDictionary to the second columnCostDictionary we initialized, otherwise the default dictionary will be columnRegularDictionary. After initializing the dictionary, we will check to see if the argument for category exists in the dictionaries we initialized previously. If category is not in columnDictionary we will have the function exit and spit a message telling the user they made a misspelling. . Once we finish the verification, we will set the plot specific setttings. The first if statement will check if categoryCase (the first letters of all words capitalized), is in the dictionary categoryDictionary. If categoryCase matches Meat, Fish, or Non_Meat, we will have it plot only for the specifc protein product. Otherwise, we will use the settings for a graph containing all protein products. . With the plot specifc settings complete, we can set the general plot settings and return the plot object. Outside of the function, we will use the .show() command to display the plot we generated. I left the .show() outside of the function so that I could prevent myself from running out of cpu. . def plotNutrition(df, category, columnTitle): &quot;&quot;&quot; This script will plot a graph from a the following variables df, dataframe to be plotted category, the following arguments are allowed: Meat Fish Non_Meat ColumnTitle , columns allowed to be plotted &quot;&quot;&quot; # Variable initialization skipCheck = False categoryCase = category.title() columnTitleCase = columnTitle.title() # Dictionaries to reference categoryDictionary = {&quot;Meat&quot;: &quot;Meat, raw/unprepared unless noted otherwise&quot;, &quot;Fish&quot;: &quot;Fish, raw/unprepared&quot;, &quot;Non_Meat&quot; : &quot;Non-meat, raw/unprepared&quot;} columnRegularDictionary = {&quot;Energy Value&quot; : &quot;Energy value (kcal)&quot;, &quot;Protein&quot;: &quot;Protein (g)&quot;, &quot;Fat&quot; : &quot;Fat (g)&quot;, &quot;Saturated Fat&quot; : &quot;Saturated fat (g)&quot;, &quot;Cholesterol&quot; : &quot;Cholesterol (mg)&quot;, &quot;Vitamin&quot; : &quot;Vitamin B12 (mcg)&quot;, &quot;Sodium&quot; :&quot;Na (mg)&quot;, &quot;Phosphorus&quot; : &quot;P (mg)&quot;, &quot;Iron&quot; : &quot;Fe (mg)&quot;, &quot;Zinc&quot; : &quot;Zn (mg)&quot;} columnCostDictionary = {&quot;Retail Cost&quot; : &quot;Retail Cost/100 xa0g (US$)&quot;, &quot;Energy Value&quot; : &quot;Energy value (kcal/US$)&quot;, &quot;Protein&quot; : &quot;Protein (g/US$)&quot;, &quot;Vitamin&quot; : &quot;Vitamin B12 (mcg/US$)&quot;, &quot;Phosphorus&quot; : &quot;P (mg/US$)&quot;, &quot;Iron&quot; : &quot;Fe (mg/US$)&quot;, &quot;Zinc&quot; : &quot;Zn (mg/US$)&quot; } # Check if which dataframe we are using, if there is &quot;Retail Cost/100 g (US$)&quot;, we will use the cost dictionary allList = df.columns.tolist() if(&quot;Retail Cost/100 xa0g (US$)&quot; in allList): columnDictionary = columnCostDictionary else: columnDictionary = columnRegularDictionary # Check if the arguement for category is exists in the dictionary, if it is not in the dictionary, it will exit the function if(categoryCase == &quot;All&quot;): skipCheck = True if((categoryCase not in categoryDictionary) &amp; (skipCheck != True)): return print(&quot;spelling error in category argument, your category argument was &quot; + categoryCase) # Initialize all dataframes needed for plotting, if the category name is inside of the category dictionary, we change the plot settings to focus only on the category name if(categoryCase in categoryDictionary): df_specific = df[df[&quot;Category&quot;] == categoryDictionary[categoryCase]].sort_values(by = [columnDictionary[columnTitleCase]]) objects = df_specific[&quot;Product&quot;] performance = df_specific[columnDictionary[columnTitleCase]] else: df = df.sort_values(by = [columnDictionary[columnTitleCase]]) objects = df[&quot;Product&quot;] performance = df[columnDictionary[columnTitleCase]] plt.tick_params(axis=&#39;y&#39;, which=&#39;major&#39;, labelsize=6) #plt.tick_params(axis=&#39;x&#39;, which=&#39;major&#39;, labelsize=6) #Plotting settings y_pos = np.arange(len(objects)) plt.barh(y_pos, performance, align=&#39;center&#39;, alpha=0.5) plt.yticks(y_pos, objects) plt.xticks(rotation=45) plt.xlabel(columnDictionary[columnTitleCase]) plt.title(categoryCase + &#39; &#39; + columnTitleCase + &#39; Comparison&#39;) plt.figure(dpi=900) return plt . Charts . After all the work, Behold! We have the final product . All Energy Values Comparison . plotNutrition(df1, &quot;all&quot;, &quot;energy value&quot;).show() plotNutrition(df1, &quot;meat&quot;, &quot;energy value&quot;).show() plotNutrition(df1, &quot;fish&quot;, &quot;energy value&quot;).show() plotNutrition(df1, &quot;non_meat&quot;, &quot;energy value&quot;).show() . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . All Protein Values Comparison . plotNutrition(df1, &quot;all&quot;, &quot;protein&quot;).show() plotNutrition(df1, &quot;meat&quot;, &quot;protein&quot;).show() plotNutrition(df1, &quot;fish&quot;, &quot;protein&quot;).show() plotNutrition(df1, &quot;non_meat&quot;, &quot;protein&quot;).show() . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . All Fat Values Comparison . plotNutrition(df1, &quot;all&quot;, &quot;fat&quot;).show() plotNutrition(df1, &quot;meat&quot;, &quot;fat&quot;).show() plotNutrition(df1, &quot;fish&quot;, &quot;fat&quot;).show() plotNutrition(df1, &quot;non_meat&quot;, &quot;fat&quot;).show() . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . All Saturated Fat Values Comparison . plotNutrition(df1, &quot;all&quot;, &quot;saturated fat&quot;).show() plotNutrition(df1, &quot;meat&quot;, &quot;saturated fat&quot;).show() plotNutrition(df1, &quot;fish&quot;, &quot;saturated fat&quot;).show() plotNutrition(df1, &quot;non_meat&quot;, &quot;saturated fat&quot;).show() . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . All Cholesterol Values Comparison . plotNutrition(df1, &quot;all&quot;, &quot;Cholesterol&quot;).show() plotNutrition(df1, &quot;meat&quot;, &quot;Cholesterol&quot;).show() plotNutrition(df1, &quot;fish&quot;, &quot;Cholesterol&quot;).show() plotNutrition(df1, &quot;non_meat&quot;, &quot;Cholesterol&quot;).show() . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . All Vitamn Values Comparison . plotNutrition(df1, &quot;all&quot;, &quot;Vitamin&quot;).show() plotNutrition(df1, &quot;meat&quot;, &quot;Vitamin&quot;).show() plotNutrition(df1, &quot;fish&quot;, &quot;Vitamin&quot;).show() plotNutrition(df1, &quot;non_meat&quot;, &quot;Vitamin&quot;).show() . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . All Sodium Values Comparison . plotNutrition(df1, &quot;all&quot;, &quot;Sodium&quot;).show() plotNutrition(df1, &quot;meat&quot;, &quot;Sodium&quot;).show() plotNutrition(df1, &quot;fish&quot;, &quot;Sodium&quot;).show() plotNutrition(df1, &quot;non_meat&quot;, &quot;Sodium&quot;).show() . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . All Phosphorus Values Comparison . plotNutrition(df1, &quot;all&quot;, &quot;Phosphorus&quot;).show() plotNutrition(df1, &quot;meat&quot;, &quot;Phosphorus&quot;).show() plotNutrition(df1, &quot;fish&quot;, &quot;Phosphorus&quot;).show() plotNutrition(df1, &quot;non_meat&quot;, &quot;Phosphorus&quot;).show() . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . All Iron Values Comparison . plotNutrition(df1, &quot;all&quot;, &quot;Iron&quot;).show() plotNutrition(df1, &quot;meat&quot;, &quot;Iron&quot;).show() plotNutrition(df1, &quot;fish&quot;, &quot;Iron&quot;).show() plotNutrition(df1, &quot;non_meat&quot;, &quot;Iron&quot;).show() . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . All Zinc Values Comparison . plotNutrition(df1, &quot;all&quot;, &quot;Zinc&quot;).show() plotNutrition(df1, &quot;meat&quot;, &quot;Zinc&quot;).show() plotNutrition(df1, &quot;fish&quot;, &quot;Zinc&quot;).show() plotNutrition(df1, &quot;non_meat&quot;, &quot;Zinc&quot;).show() . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . All Retail Cost Comparison . allRetailCost = [&quot;retail cost&quot;, &quot;energy value&quot;, &quot;protein&quot;, &quot;vitamin&quot;, &quot;phosphorus&quot;, &quot;iron&quot;, &quot;zinc&quot;] for i in range(len(allRetailCost)): plotNutrition(df2, &quot;all&quot;, allRetailCost[i]).show() plotNutrition(df2, &quot;meat&quot;, allRetailCost[i]).show() plotNutrition(df2, &quot;fish&quot;, allRetailCost[i]).show() plotNutrition(df2, &quot;non_meat&quot;, allRetailCost[i]).show() . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . /home/bedhedd/miniconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 150 missing from current font. font.set_text(s, 0.0, flags=flags) /home/bedhedd/miniconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 150 missing from current font. font.load_char(ord(s), flags=flags) . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . &lt;Figure size 5760x4320 with 0 Axes&gt; . Testing . This section are tests I made to check dictionaries and lists outside of the function . Initializing Test Variables . test1 = False test2 = False name = &quot;meat&quot; name2 = &quot;Saturated Fat&quot; ref2 = {&quot;Meat&quot;: &quot;Meat, raw/unprepared unless noted otherwise&quot;, &quot;Fish&quot;: &quot;Fish, raw/unprepared&quot;, &quot;Non_meat&quot; : &quot;Non-meat, raw/unprepared&quot;} if(name.title() in ref3): test1 = True print(test1) . False . Testing if Saturated Fat Exists in ref4 Dictonary . ref = {&quot;meat&quot;: &quot;Meat, raw/unprepared unless noted otherwise&quot;, &quot;fish&quot;: &quot;Fish, raw/unprepared&quot;, &quot;non_meat&quot; : &quot;Non-meat, raw/unprepared&quot;} ref1 = {meat: &quot;Meat&quot;, fish: &quot;Fish&quot;, non_meat: &quot;Non_meat&quot;} ref2 = {&quot;Meat, raw/unprepared unless noted otherwise&quot; : &quot;meat&quot;, &quot;Fish&quot;: &quot;Fish, raw/unprepared&quot;, &quot;Non_meat&quot; : &quot;Non-meat, raw/unprepared&quot;} ref3 = {&quot;retail cost&quot; : &quot;Retail Cost/100 g (US$)&quot;, &quot;energy value&quot; : &quot;Energy value (kcal/US$)&quot;, &quot;protein&quot; : &quot;Protein (g/US$)&quot;, &quot;vitamin&quot; : &quot;Vitamin B12 (mcg/US$)&quot;, &quot;phosphorus&quot; : &quot;P (mg/US$)&quot;, &quot;iron&quot; : &quot;Fe (mg/US$)&quot;, &quot;zinc&quot; : &quot;Zn (mg/US$)&quot; } ref4 = {&quot;Energy Value&quot; : &quot;Energy value (kcal)&quot;, &quot;Protein&quot;: &quot;Protein (g)&quot;, &quot;Fat&quot; : &quot;Fat (g)&quot;, &quot;Saturated Fat&quot;: &quot;Saturated fat (g)&quot;, &quot;Cholesterol&quot; : &quot;Cholesterol (mg)&quot;, &quot;Vitamin B12&quot; &quot;Vitamin B12 (mcg)&quot; &quot;Sodium&quot; :&quot;Na (mg)&quot;, &quot;Phosphorus&quot; : &quot;P (mg)&quot;, &quot;Iron&quot; : &quot;Fe (mg)&quot;, &quot;Zinc&quot; : &quot;Zn (mg)&quot;} #ref1[&quot;Meat, raw/unprepared unless noted otherwise&quot;] ref[&quot;meat&quot;] ref3[&quot;energy Value&quot;.casefold()] ref4[&quot;Saturated Fat&quot;] if(name2.title() in ref4): test1 = True print(test1) . True . Testing Dictionary Outputs In Relation to Variables . category = &quot;meat&quot; columnTitle = &quot;retail cost&quot; categoryCase = category.casefold() columnTitleCase = columnTitle.casefold() categoryDictionary = {&quot;meat&quot;: &quot;Meat, raw/unprepared unless noted otherwise&quot;, &quot;fish&quot;: &quot;Fish, raw/unprepared&quot;, &quot;non_meat&quot; : &quot;Non-meat, raw/unprepared&quot;} columnDictionary = {&quot;retail cost&quot; : &quot;Retail Cost/100 g (US$)&quot;, &quot;energy value&quot; : &quot;Energy value (kcal/US$)&quot;, &quot;protein&quot; : &quot;Protein (g/US$)&quot;, &quot;vitamin&quot; : &quot;Vitamin B12 (mcg/US$)&quot;, &quot;phosphorus&quot; : &quot;P (mg/US$)&quot;, &quot;iron&quot; : &quot;Fe (mg/US$)&quot;, &quot;zinc&quot; : &quot;Zn (mg/US$)&quot; } #df_specific = df2[df2[&quot;Category&quot;] == categoryDictionary[categoryCase]].sort_values(by = [columnDictionary[columnTitleCase]]) #df_specific columnDictionary[columnTitleCase] . &#39;Retail Cost/100 g (US$)&#39; . Testing Solution To Backslashes Causing Issues in Python . allList = df2.columns.tolist() if(&quot;Retail Cost/100 xa0g (US$)&quot; in allList): print(&quot;worked&quot;) else: print(&quot;didn&#39;t work&quot;) allList . worked . [&#39;Category&#39;, &#39;Product&#39;, &#39;Retail Cost/100 xa0g (US$)&#39;, &#39;Source of estimationb&#39;, &#39;Energy value (kcal/US$)&#39;, &#39;Protein (g/US$)&#39;, &#39;Vitamin B12 (mcg/US$)&#39;, &#39;P (mg/US$)&#39;, &#39;Fe (mg/US$)&#39;, &#39;Zn (mg/US$)&#39;] . p= [] p.append(ref) df3 = pd.DataFrame(p) df3 if(name in p): test1 = . File &#34;&lt;ipython-input-526-f7fecc95bb11&gt;&#34;, line 8 test1 = ^ SyntaxError: invalid syntax . Previous versions of functions . These are previous iterations of the plotNutrition() function, I decided to keep them at the end in case you were curious about the evolution of nutrition function. When I started this project, I was had a early deadline. I started with a longer function so that could plot graphs for specific columns. . At the time, I did not realize that lines 11-15 were relatively the same. I also was not very comfortable with substituting dictionary entries for strings. When I had more time, I started to work on making the plot function more modular. In the final product you will notice that I have formal dictionaries calls. . Dictionary calls make the code more modular, but they also abstract the code more. If you are not careful with keeping track of your dictionary calls, diagnosing errors in the code become much harder . Energy Values Plotting Function . def plotEV(df, name): ref = {&quot;meat&quot;: &quot;Meat, raw/unprepared unless noted otherwise&quot;, &quot;fish&quot;: &quot;Fish, raw/unprepared&quot;, &quot;non_meat&quot; : &quot;Non-meat, raw/unprepared&quot;} caseInsensitiveName = name.casefold() if(caseInsensitiveName in ref): df_specific = df[df1[&quot;Category&quot;] == ref[caseInsensitiveName]].nsmallest(n = len(df), columns = &quot;Energy value (kcal)&quot;) objects = df_specific[&quot;Product&quot;] y_pos = np.arange(len(objects)) performance = df_specific[&quot;Energy value (kcal)&quot;] plt.barh(y_pos, performance, align=&#39;center&#39;, alpha=0.5) plt.yticks(y_pos, objects) plt.xlabel(&#39;Energy Value (kcal)&#39;) plt.title(name.title() + &#39; Energy Values Comparison&#39;) plt.figure(dpi=300) else: df = df.nsmallest(n = len(df), columns = &quot;Energy value (kcal)&quot;) objects = df[&quot;Product&quot;] y_pos = np.arange(len(objects)) performance = df[&quot;Energy value (kcal)&quot;] plt.barh(y_pos, performance, align=&#39;center&#39;, alpha=0.5) plt.yticks(y_pos, objects) plt.xlabel(&#39;Energy Value (kcal)&#39;) plt.title(&#39;All Energy Values Comparison&#39;) plt.tick_params(axis=&#39;y&#39;, which=&#39;major&#39;, labelsize=6) plt.figure(dpi=300) return plt . Protein Values Plotting Function . def plotProtein(df, name): ref = {&quot;meat&quot;: &quot;Meat, raw/unprepared unless noted otherwise&quot;, &quot;fish&quot;: &quot;Fish, raw/unprepared&quot;, &quot;non_meat&quot; : &quot;Non-meat, raw/unprepared&quot;} if(name in ref): df_specific = df[df1[&quot;Category&quot;] == ref[name]].nsmallest(n = len(df), columns = &quot;Protein (g)&quot;) objects = df_specific[&quot;Product&quot;] y_pos = np.arange(len(objects)) performance = df_specific[&quot;Protein (g)&quot;] plt.barh(y_pos, performance, align=&#39;center&#39;, alpha=0.5) plt.yticks(y_pos, objects) plt.xlabel(&#39;Protein (g)&#39;) plt.title(name.title( + &#39; Protein Values Comparison&#39;) plt.figure(dpi=300) else: df = df.nsmallest(n = len(df), columns = &quot;Protein (g)&quot;) objects = df[&quot;Product&quot;] y_pos = np.arange(len(objects)) performance = df[&quot;Protein (g)&quot;] plt.barh(y_pos, performance, align=&#39;center&#39;, alpha=0.5) plt.yticks(y_pos, objects) plt.xlabel(&#39;Protein (g)&#39;) plt.title(&#39;All Protein Values Comparison&#39;) plt.tick_params(axis=&#39;y&#39;, which=&#39;major&#39;, labelsize=5) plt.figure(dpi=300) return plt . File &#34;&lt;ipython-input-10-ba8e2741f3b2&gt;&#34;, line 13 plt.figure(dpi=300) ^ SyntaxError: invalid syntax .",
            "url": "https://progressedd.github.io/blog/2020/04/19/Nutrition-analysis.html",
            "relUrl": "/2020/04/19/Nutrition-analysis.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://progressedd.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Analysis on the Pros of Overwatch",
            "content": "import pandas as pd from IPython.core.display import display, HTML display(HTML(&quot;&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;&quot;)) . Loading the Data Frame . I used the pandas library to import my csv file to a Dataframe . df = pd.read_csv(&quot;owl.csv&quot;) df . playerId teamId role name team eliminations_avg_per_10m deaths_avg_per_10m hero_damage_avg_per_10m healing_avg_per_10m ultimates_earned_avg_per_10m final_blows_avg_per_10m time_played_total count . 0 8675 | 7693 | offense | Adora | HZS | 16.002129 | 3.876235 | 5390.964430 | 5678.864360 | 4.410253 | 4.134037 | 65166.328327 | 1 | . 1 3985 | 4523 | offense | aKm | DAL | 17.296143 | 5.178533 | 9600.623636 | 158.646056 | 4.575825 | 6.050875 | 75658.485169 | 1 | . 2 8812 | 7692 | tank | ameng | CDH | 16.470544 | 6.202665 | 7051.640852 | 0.724798 | 4.511644 | 5.046006 | 88703.810417 | 1 | . 3 4841 | 4403 | support | Anamo | NYE | 14.165021 | 2.745207 | 3948.127250 | 6563.244540 | 3.567532 | 1.898150 | 97041.863429 | 1 | . 4 5715 | 4404 | offense | Architect | SFS | 18.181756 | 4.555888 | 7825.862117 | 3806.894747 | 4.639483 | 8.087747 | 28710.098079 | 1 | . 5 4652 | 7697 | support | ArK | WAS | 7.870130 | 2.641469 | 2482.555936 | 6825.268613 | 3.518943 | 1.320735 | 66326.727702 | 1 | . 6 5878 | 4402 | tank | Axxiom | BOS | 13.589015 | 6.323052 | 7227.732159 | 0.000000 | 3.771645 | 3.383387 | 10817.561445 | 1 | . 7 4491 | 7698 | offense | babybay | ATL | 18.285137 | 4.898535 | 9724.894013 | 153.512705 | 4.520151 | 6.320031 | 58670.601443 | 1 | . 8 8896 | 7692 | offense | Baconjack | CDH | 15.605830 | 6.119487 | 7305.171594 | 123.938222 | 4.231318 | 6.506221 | 26374.758866 | 1 | . 9 4624 | 4525 | support | Bani | HOU | 11.303448 | 5.257418 | 4791.691330 | 6825.377770 | 3.680192 | 2.102967 | 2282.489336 | 1 | . 10 8676 | 7693 | offense | Bazzi | HZS | 15.814585 | 5.846427 | 5369.937012 | 7506.352638 | 4.589445 | 3.800178 | 20525.356901 | 1 | . 11 4655 | 4410 | support | Bdosin | LDN | 15.048464 | 4.107217 | 6983.868463 | 8836.736647 | 4.746420 | 3.882816 | 88234.919687 | 1 | . 12 8731 | 7693 | support | BEBE | HZS | 14.154976 | 3.952661 | 7197.340358 | 9429.701829 | 4.803399 | 3.533836 | 91685.070527 | 1 | . 13 8708 | 7694 | tank | BenBest | PAR | 13.411352 | 6.584489 | 6648.912608 | 0.000000 | 4.201147 | 4.026099 | 44559.264875 | 1 | . 14 4643 | 4406 | support | BigG00se | GLA | 12.350082 | 4.683845 | 3870.859580 | 6429.321595 | 3.655210 | 2.206065 | 92744.320062 | 1 | . 15 4639 | 7699 | tank | Bischu | GZC | 16.794393 | 5.927433 | 7229.213813 | 1836.443127 | 2.963716 | 8.891149 | 1214.691094 | 1 | . 16 8184 | 4525 | offense | blase | HOU | 14.115152 | 5.688758 | 5288.563477 | 5127.842249 | 3.993433 | 4.545983 | 95556.889781 | 1 | . 17 4625 | 4525 | support | Boink | HOU | 8.675604 | 5.084554 | 2736.000684 | 5754.139120 | 3.133041 | 1.772292 | 90391.403033 | 1 | . 18 4079 | 4524 | support | Boombox | PHI | 14.716397 | 4.686374 | 6282.962865 | 9251.731158 | 4.765247 | 3.542715 | 91285.929199 | 1 | . 19 8704 | 4407 | offense | bqb | FLA | 16.439333 | 4.809016 | 8641.233568 | 319.525665 | 4.536164 | 4.972728 | 87959.775570 | 1 | . 20 8771 | 7696 | tank | BUMPER | VAN | 16.630474 | 5.906248 | 7969.093625 | 6.324790 | 4.654208 | 5.399093 | 113574.634219 | 1 | . 21 10031 | 4407 | support | byrem | FLA | 11.418219 | 5.360666 | 5266.262933 | 7719.478526 | 3.752466 | 4.127713 | 11192.639166 | 1 | . 22 4620 | 4524 | offense | carpe | PHI | 17.983018 | 4.844880 | 8782.867900 | 137.704463 | 4.146629 | 7.031507 | 97959.083057 | 1 | . 23 8721 | 7699 | support | Chara | GZC | 10.361039 | 3.866669 | 3932.990917 | 5914.067038 | 3.253993 | 1.912912 | 88137.878443 | 1 | . 24 5831 | 4404 | tank | Choihyobin | SFS | 19.391551 | 3.472553 | 6035.142896 | 302.338209 | 2.882165 | 5.957548 | 111790.956037 | 1 | . 25 4663 | 4523 | support | Closer | DAL | 10.935586 | 3.527819 | 3511.197807 | 6354.660012 | 3.377837 | 1.936714 | 92011.533219 | 1 | . 26 5306 | 4402 | offense | ColourHex | BOS | 14.899417 | 5.548090 | 8649.812908 | 168.870374 | 3.966816 | 5.752565 | 88030.291299 | 1 | . 27 8892 | 4408 | support | CoMa | SHD | 7.909984 | 3.700223 | 2070.655671 | 6161.834635 | 3.184618 | 1.389100 | 98912.968913 | 1 | . 28 4138 | 4525 | tank | coolmatt | HOU | 15.208515 | 4.527089 | 6774.211482 | 183.137562 | 4.214156 | 4.777435 | 28760.204940 | 1 | . 29 8784 | 7697 | offense | Corey | WAS | 15.025158 | 5.201795 | 9003.292089 | 127.436107 | 4.608075 | 7.036929 | 88930.845029 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 123 9155 | 7693 | offense | SASIN | HZS | 15.217982 | 4.972212 | 6304.243753 | 450.467436 | 2.862789 | 6.026923 | 3982.131229 | 1 | . 124 5442 | 4407 | offense | sayaplayer | FLA | 13.396877 | 5.487655 | 6048.630900 | 4204.440753 | 4.051289 | 6.067726 | 65164.442533 | 1 | . 125 8728 | 7696 | offense | SeoMinSoo | VAN | 21.092578 | 3.588644 | 10933.062491 | 198.216055 | 5.075465 | 7.048881 | 88780.044028 | 1 | . 126 10041 | 4405 | offense | Shax | VAL | 16.920446 | 5.694832 | 6499.728449 | 332.208914 | 4.148087 | 6.515075 | 25602.161674 | 1 | . 127 4644 | 4406 | support | Shaz | GLA | 14.269397 | 4.063945 | 6467.514174 | 10113.314711 | 4.988753 | 3.243341 | 92127.225783 | 1 | . 128 8725 | 7699 | support | shu | GZC | 14.059122 | 3.826302 | 6458.460594 | 8793.613298 | 4.767641 | 3.609592 | 88597.283044 | 1 | . 129 4139 | 4404 | offense | sinatraa | SFS | 20.914314 | 3.497822 | 11849.397959 | 111.562321 | 5.252785 | 7.019851 | 99147.405428 | 1 | . 130 8749 | 7696 | support | SLIME | VAN | 14.527621 | 3.703972 | 4165.190298 | 5882.757861 | 3.192704 | 2.724949 | 110314.003005 | 1 | . 131 7831 | 4404 | tank | Smurf | SFS | 17.720797 | 4.922444 | 6587.333835 | 0.000000 | 3.999486 | 4.983974 | 19502.508479 | 1 | . 132 4493 | 4524 | offense | snillo | PHI | 13.115663 | 7.286479 | 4268.438247 | 6369.452283 | 2.914592 | 2.914592 | 411.721471 | 1 | . 133 3987 | 7694 | offense | SoOn | PAR | 17.156117 | 6.122379 | 8755.638934 | 181.999070 | 4.459629 | 6.852836 | 62426.713501 | 1 | . 134 4637 | 4406 | tank | SPACE | GLA | 16.937870 | 3.754919 | 6297.654849 | 349.497358 | 4.028947 | 5.421303 | 81013.727587 | 1 | . 135 4094 | 4525 | tank | SPREE | HOU | 16.057987 | 4.951975 | 7074.346063 | 442.661626 | 3.723781 | 5.265556 | 45921.072883 | 1 | . 136 8747 | 7696 | offense | Stitch | VAN | 17.654422 | 5.264495 | 7419.042221 | 628.318125 | 4.211596 | 6.807114 | 24503.776509 | 1 | . 137 8783 | 7697 | offense | Stratus | WAS | 15.851749 | 5.322345 | 6520.332819 | 1105.055288 | 3.785206 | 5.648987 | 31226.837463 | 1 | . 138 4610 | 4404 | offense | STRIKER | SFS | 16.207208 | 6.053294 | 7380.909425 | 140.294067 | 4.491154 | 8.201238 | 6145.413898 | 1 | . 139 4490 | 4404 | tank | super | SFS | 17.351664 | 5.256228 | 8277.983328 | 5.826254 | 4.657262 | 6.197460 | 98169.257876 | 1 | . 140 4144 | 4406 | offense | Surefour | GLA | 16.264683 | 4.850343 | 7702.970198 | 2334.307256 | 4.409403 | 6.784467 | 59872.055648 | 1 | . 141 3380 | 4523 | offense | Taimou | DAL | 13.643140 | 5.311080 | 6980.864470 | 353.580132 | 4.141668 | 6.724119 | 12313.880814 | 1 | . 142 4104 | 4409 | support | tobi | SEO | 12.527639 | 3.561285 | 3821.983777 | 6536.903535 | 3.460255 | 2.083730 | 47510.945922 | 1 | . 143 10021 | 4523 | tank | Trill | DAL | 11.016714 | 6.549455 | 5723.625044 | 0.000000 | 3.482947 | 3.369373 | 15848.646641 | 1 | . 144 8729 | 7696 | support | Twilight | VAN | 16.044986 | 4.139107 | 7122.262343 | 9320.446421 | 5.048997 | 4.156948 | 100891.334471 | 1 | . 145 3983 | 4523 | support | uNKOE | DAL | 12.889446 | 4.796602 | 5795.335146 | 9419.885852 | 4.592007 | 3.175002 | 79181.059063 | 1 | . 146 8688 | 4404 | support | Viol2t | SFS | 17.185764 | 4.195764 | 7637.591409 | 10409.200261 | 5.300486 | 4.440653 | 110254.045210 | 1 | . 147 5809 | 4406 | tank | Void | GLA | 19.343389 | 3.178936 | 6457.220391 | 250.965632 | 2.048810 | 5.555847 | 82291.682828 | 1 | . 148 8710 | 7692 | offense | YangXiaoLong | CDH | 15.342481 | 5.957453 | 7572.263394 | 299.683611 | 4.115422 | 6.295547 | 51464.949743 | 1 | . 149 8894 | 4408 | offense | YOUNGJIN | SHD | 17.674387 | 5.835630 | 6183.788216 | 5143.003544 | 4.046751 | 6.204125 | 89553.317851 | 1 | . 150 8813 | 7692 | support | Yveltal | CDH | 7.512112 | 4.692623 | 2209.111332 | 7691.353637 | 3.602681 | 1.520697 | 91931.532277 | 1 | . 151 8642 | 4523 | offense | ZachaREEE | DAL | 15.167823 | 5.666740 | 5698.414532 | 6211.639093 | 4.642945 | 3.866947 | 92010.565901 | 1 | . 152 4098 | 4409 | tank | ZUNBA | SEO | 18.577579 | 2.959663 | 6789.544551 | 0.000000 | 2.549864 | 5.737194 | 13177.174514 | 1 | . 153 rows × 13 columns . Relations Between Stats . We are going to make a correlation table with the .corr() Function . df_correleations = df[[&quot;eliminations_avg_per_10m&quot;,&quot;deaths_avg_per_10m&quot;,&quot;hero_damage_avg_per_10m&quot;,&quot;healing_avg_per_10m&quot;,&quot;ultimates_earned_avg_per_10m&quot;,&quot;final_blows_avg_per_10m&quot;,&quot;time_played_total&quot;]] df_corr_table = df_correleations.corr() df_corr_table . eliminations_avg_per_10m deaths_avg_per_10m hero_damage_avg_per_10m healing_avg_per_10m ultimates_earned_avg_per_10m final_blows_avg_per_10m time_played_total . eliminations_avg_per_10m 1.000000 | -0.053546 | 0.702982 | -0.520182 | 0.108123 | 0.768385 | 0.184809 | . deaths_avg_per_10m -0.053546 | 1.000000 | 0.157248 | -0.242843 | 0.131407 | 0.183793 | -0.312624 | . hero_damage_avg_per_10m 0.702982 | 0.157248 | 1.000000 | -0.468124 | 0.467872 | 0.756272 | 0.130058 | . healing_avg_per_10m -0.520182 | -0.242843 | -0.468124 | 1.000000 | 0.345956 | -0.662879 | 0.021491 | . ultimates_earned_avg_per_10m 0.108123 | 0.131407 | 0.467872 | 0.345956 | 1.000000 | 0.100654 | 0.123582 | . final_blows_avg_per_10m 0.768385 | 0.183793 | 0.756272 | -0.662879 | 0.100654 | 1.000000 | 0.007640 | . time_played_total 0.184809 | -0.312624 | 0.130058 | 0.021491 | 0.123582 | 0.007640 | 1.000000 | . This is great, but let&#39;s filter out correlations below .5 because they aren&#39;t strong . df_corr_table[(df_corr_table&gt; .5) | (df_corr_table &lt;-.5) &amp; (df_corr_table &lt; 1.0)] . eliminations_avg_per_10m deaths_avg_per_10m hero_damage_avg_per_10m healing_avg_per_10m ultimates_earned_avg_per_10m final_blows_avg_per_10m time_played_total . eliminations_avg_per_10m 1.000000 | NaN | 0.702982 | -0.520182 | NaN | 0.768385 | NaN | . deaths_avg_per_10m NaN | 1.0 | NaN | NaN | NaN | NaN | NaN | . hero_damage_avg_per_10m 0.702982 | NaN | 1.000000 | NaN | NaN | 0.756272 | NaN | . healing_avg_per_10m -0.520182 | NaN | NaN | 1.000000 | NaN | -0.662879 | NaN | . ultimates_earned_avg_per_10m NaN | NaN | NaN | NaN | 1.0 | NaN | NaN | . final_blows_avg_per_10m 0.768385 | NaN | 0.756272 | -0.662879 | NaN | 1.000000 | NaN | . time_played_total NaN | NaN | NaN | NaN | NaN | NaN | 1.0 | . this looks great, but we are going to make this cleaner with the .drop() function so that the table doesn&#39;t have so many NaNs . df_column_drop = df_corr_table.drop([&quot;deaths_avg_per_10m&quot;, &quot;ultimates_earned_avg_per_10m&quot;, &quot;time_played_total&quot;]) df_relevant_correlations = df_column_drop.drop([&quot;deaths_avg_per_10m&quot;, &quot;ultimates_earned_avg_per_10m&quot;, &quot;time_played_total&quot;],axis=1) df_relevant_correlations . eliminations_avg_per_10m hero_damage_avg_per_10m healing_avg_per_10m final_blows_avg_per_10m . eliminations_avg_per_10m 1.000000 | 0.702982 | -0.520182 | 0.768385 | . hero_damage_avg_per_10m 0.702982 | 1.000000 | -0.468124 | 0.756272 | . healing_avg_per_10m -0.520182 | -0.468124 | 1.000000 | -0.662879 | . final_blows_avg_per_10m 0.768385 | 0.756272 | -0.662879 | 1.000000 | . it appears that . Eliminations and Hero Damage have a strong positive correlation with each other . Eliminations and Final blows have a strong positive correlation with each other . Eliminations and Healing have a moderate negative correlation with each other . Final blows and Hero damage have a strong positive correlation with each other . Final blows and Healing have a moderate negative correlation with each other . Figuring Out Which Pro has The Best Stats . Let&#39;s take a look at which Pros have the most eliminations, damage dealt, final blows, ultimates earned, healing dealt, and least number of deaths . Most Eliminations . df_elims = df[[&quot;role&quot;, &quot;name&quot;, &quot;team&quot;, &quot;eliminations_avg_per_10m&quot;]] df_elims.nlargest(10, &quot;eliminations_avg_per_10m&quot;) . role name team eliminations_avg_per_10m . 125 offense | SeoMinSoo | VAN | 21.092578 | . 129 offense | sinatraa | SFS | 20.914314 | . 75 tank | JJANU | VAN | 20.056908 | . 24 tank | Choihyobin | SFS | 19.391551 | . 98 offense | Nenne | NYE | 19.365667 | . 147 tank | Void | GLA | 19.343389 | . 59 offense | Haksal | VAN | 19.341104 | . 35 offense | diem | SHD | 19.069305 | . 93 tank | Michelle | SEO | 18.928176 | . 43 offense | FITS | SEO | 18.882519 | . Most Damage Dealt . df_damage = df[[&quot;role&quot;, &quot;name&quot;, &quot;team&quot;, &quot;hero_damage_avg_per_10m&quot;]] df_damage.nlargest(10, &quot;hero_damage_avg_per_10m&quot;) . role name team hero_damage_avg_per_10m . 129 offense | sinatraa | SFS | 11849.397959 | . 125 offense | SeoMinSoo | VAN | 10933.062491 | . 36 offense | Diya | SHD | 10081.816984 | . 69 offense | ivy | TOR | 9899.923990 | . 7 offense | babybay | ATL | 9724.894013 | . 56 offense | GodsB | HZS | 9683.685510 | . 1 offense | aKm | DAL | 9600.623636 | . 43 offense | FITS | SEO | 9527.150428 | . 98 offense | Nenne | NYE | 9397.008782 | . 116 support | Ripa | GLA | 9261.831927 | . Most Final Blows Dealt . df_final_blows = df[[&quot;role&quot;, &quot;name&quot;, &quot;team&quot;, &quot;final_blows_avg_per_10m&quot;]] df_final_blows.nlargest(10, &quot;final_blows_avg_per_10m&quot;) . role name team final_blows_avg_per_10m . 15 tank | Bischu | GZC | 8.891149 | . 87 offense | Logix | TOR | 8.368265 | . 138 offense | STRIKER | SFS | 8.201238 | . 4 offense | Architect | SFS | 8.087747 | . 35 offense | diem | SHD | 7.917622 | . 86 offense | LiNkzr | HOU | 7.424964 | . 45 tank | fragi | GZC | 7.409291 | . 36 offense | Diya | SHD | 7.223194 | . 99 offense | nero | GZC | 7.065229 | . 125 offense | SeoMinSoo | VAN | 7.048881 | . Most Ultimates Earned . df_ultimates = df[[&quot;role&quot;, &quot;name&quot;, &quot;team&quot;, &quot;ultimates_earned_avg_per_10m&quot;]] df_ultimates.nlargest(10, &quot;ultimates_earned_avg_per_10m&quot;) . role name team ultimates_earned_avg_per_10m . 116 support | Ripa | GLA | 6.299542 | . 110 support | RAPEL | VAN | 5.533913 | . 146 support | Viol2t | SFS | 5.300486 | . 129 offense | sinatraa | SFS | 5.252785 | . 78 support | Krillin | LDN | 5.159678 | . 111 offense | Rascal | SFS | 5.150937 | . 70 support | IZaYaKI | SHD | 5.089731 | . 125 offense | SeoMinSoo | VAN | 5.075465 | . 144 support | Twilight | VAN | 5.048997 | . 81 offense | Krystal | HZS | 5.003493 | . Most Healing Dealt . df_heals = df[[&quot;role&quot;, &quot;name&quot;, &quot;team&quot;, &quot;healing_avg_per_10m&quot;]] df_heals.nlargest(10, &quot;healing_avg_per_10m&quot;) . role name team healing_avg_per_10m . 116 support | Ripa | GLA | 12489.082153 | . 61 support | HarryHook | DAL | 10731.194372 | . 70 support | IZaYaKI | SHD | 10720.448760 | . 110 support | RAPEL | VAN | 10575.563042 | . 66 support | HyP | PAR | 10568.360507 | . 78 support | Krillin | LDN | 10410.303999 | . 146 support | Viol2t | SFS | 10409.200261 | . 127 support | Shaz | GLA | 10113.314711 | . 12 support | BEBE | HZS | 9429.701829 | . 145 support | uNKOE | DAL | 9419.885852 | . Least Deaths Recieved . df_deaths = df[[&quot;role&quot;, &quot;name&quot;, &quot;team&quot;, &quot;deaths_avg_per_10m&quot;]] df_deaths.nsmallest(10, &quot;deaths_avg_per_10m&quot;) . role name team deaths_avg_per_10m . 5 support | ArK | WAS | 2.641469 | . 3 support | Anamo | NYE | 2.745207 | . 152 tank | ZUNBA | SEO | 2.959663 | . 147 tank | Void | GLA | 3.178936 | . 93 tank | Michelle | SEO | 3.239774 | . 75 tank | JJANU | VAN | 3.381134 | . 24 tank | Choihyobin | SFS | 3.472553 | . 105 support | Onlywish | GZC | 3.493403 | . 129 offense | sinatraa | SFS | 3.497822 | . 25 support | Closer | DAL | 3.527819 | . It wouldn&#39;t be fair to judge the stats for pro&#39;s who don&#39;t get a lot of play time. . let&#39;s see which pros got the most play time . Most Playtime Recived . df_time_largest = df[[&quot;role&quot;, &quot;name&quot;, &quot;team&quot;, &quot;time_played_total&quot;]] df_time_largest.nlargest(10, &quot;time_played_total&quot;) . role name team time_played_total . 95 support | moth | SFS | 116859.144868 | . 20 tank | BUMPER | VAN | 113574.634219 | . 24 tank | Choihyobin | SFS | 111790.956037 | . 130 support | SLIME | VAN | 110314.003005 | . 146 support | Viol2t | SFS | 110254.045210 | . 59 offense | Haksal | VAN | 109848.948995 | . 75 tank | JJANU | VAN | 107005.528668 | . 111 offense | Rascal | SFS | 102156.164167 | . 144 support | Twilight | VAN | 100891.334471 | . 129 offense | sinatraa | SFS | 99147.405428 | . Least Playtime Recived . df_time_smallest = df[[&quot;role&quot;, &quot;name&quot;, &quot;team&quot;, &quot;time_played_total&quot;]] df_time_smallest.nsmallest(10, &quot;time_played_total&quot;) . role name team time_played_total . 132 offense | snillo | PHI | 411.721471 | . 77 tank | Karayan | FLA | 496.426940 | . 36 offense | Diya | SHD | 498.394445 | . 116 support | Ripa | GLA | 666.715092 | . 47 support | FunnyAstro | ATL | 1020.976662 | . 15 tank | Bischu | GZC | 1214.691094 | . 45 tank | fragi | GZC | 1214.691094 | . 53 tank | Gator | ATL | 1735.402076 | . 94 tank | Mickie | DAL | 2149.076643 | . 105 support | Onlywish | GZC | 2232.780163 | . Explanation For What I have chosen . I have choosen Eliminations, Damage, and Deaths for this project because I wanted to apply the findings to my own games. . Although these statistics do not reflect the skill level of a player, I personally would like to use these to figure out how I personally compare in my own competitive games to the pros. . Grouping by role . Which roles has the best in the various categories . df_group_role = df.groupby(&quot;role&quot;) df_group_role = df_group_role.agg(&quot;sum&quot;).reset_index() df_group_role . role playerId teamId eliminations_avg_per_10m deaths_avg_per_10m hero_damage_avg_per_10m healing_avg_per_10m ultimates_earned_avg_per_10m final_blows_avg_per_10m time_played_total count . 0 offense | 386317 | 334557 | 946.176006 | 291.759163 | 425826.207321 | 130963.348462 | 246.230313 | 341.229728 | 3.354968e+06 | 57 | . 1 support | 352231 | 289111 | 599.786627 | 214.251135 | 238364.322933 | 386289.038203 | 202.228107 | 135.218342 | 3.153147e+06 | 49 | . 2 tank | 316162 | 270656 | 768.694654 | 242.023534 | 307152.496101 | 9071.285277 | 167.693455 | 242.304149 | 2.905581e+06 | 47 | . df_group_role.plot.bar(x = &quot;role&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5a30cce470&gt; . Counts For Each Role . df_group_role[[&quot;role&quot;,&quot;count&quot;]] . role count . 0 offense | 57 | . 1 support | 49 | . 2 tank | 47 | . df_group_role.plot.bar(x = &quot;role&quot;, y = &quot;count&quot; ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5a30398550&gt; . across all the roles, the count seems to be pretty spread evenly . How Many Eliminations Do Each Role Get? . They say that DPS should be the ones who get the most eliminations. That is true, however, tanks are pretty close to DPS. . df_group_role[&quot;Overall Average Eliminations Per 10m&quot;] = df_group_role[&quot;eliminations_avg_per_10m&quot;]/df_group_role[&quot;count&quot;] df_group_role_elimminations = df_group_role[[&quot;role&quot;,&quot;Overall Average Eliminations Per 10m&quot;]] df_group_role_elimminations.nlargest(3, &quot;Overall Average Eliminations Per 10m&quot;) . role Overall Average Eliminations Per 10m . 0 offense | 16.599579 | . 2 tank | 16.355205 | . 1 support | 12.240543 | . df_group_role.plot.bar(x = &quot;role&quot;, y = &quot;Overall Average Eliminations Per 10m&quot; ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5a30360f60&gt; . it appears that The tanks and DPS get almost about the same amount of damage. The Tanks might have more than the supports because they get more opportunities to do damage and finish off targets. . It is also possible that during the GOATs meta (3 tanks 3 supports) tanks were the primary damage dealers. . How Much Damage is Dealt by Each Role? . df_group_role[&quot;Overall Average Damage Dealt per 10m&quot;] = df_group_role[&quot;hero_damage_avg_per_10m&quot;]/df_group_role[&quot;count&quot;] df_group_role_Damage = df_group_role[[&quot;role&quot;,&quot;Overall Average Damage Dealt per 10m&quot;]] df_group_role_Damage.nlargest(3, &quot;Overall Average Damage Dealt per 10m&quot;) . role Overall Average Damage Dealt per 10m . 0 offense | 7470.635216 | . 2 tank | 6535.159492 | . 1 support | 4864.578019 | . df_group_role.plot.bar(x = &quot;role&quot;, y = &quot;Overall Average Damage Dealt per 10m&quot; ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5a302e6898&gt; . In this category, the DPS did 1000 more damage per 10 min compared to the tanks. . It is possible that the dps can focus more on dealing damage thus they would have more than the tanks. If we compare this back to the eliminations, it would appear that the DPS do the majority of the damage, and the tanks would finish off low health oppponents . How Many Deaths Do Each Role Get? . df_group_role[&quot;Overall Average Deaths Per 10m&quot;] = df_group_role[&quot;deaths_avg_per_10m&quot;]/df_group_role[&quot;count&quot;] df_group_role_Deaths = df_group_role[[&quot;role&quot;,&quot;Overall Average Deaths Per 10m&quot;]] df_group_role_Deaths.nlargest(3,&quot;Overall Average Deaths Per 10m&quot;) . role Overall Average Deaths Per 10m . 2 tank | 5.149437 | . 0 offense | 5.118582 | . 1 support | 4.372472 | . df_group_role.plot.bar(x = &quot;role&quot;, y = &quot;Overall Average Deaths Per 10m&quot; ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5a302ae6d8&gt; . In terms of deaths, it appears that the dps and tanks die more than the supports. . The values are reasonable as supports in this game have better abilities that help them self sustain(Lucio amp and wall riding, Brigitte having a shield and inspire, Ana having sleep and nade, Moria healing orb and fade out etc. . DPS and tanks constantly need to put themselves in risky positions to get value and their abilities for the most part don&#39;t heal or sustain themselves (with obvious exceptions like soldier health pad, doomfist shields, and tracer recall). . How do teams affect statistics . df_group_team = df.groupby(&quot;team&quot;) df_group_team = df_group_team.agg(&quot;sum&quot;).reset_index() df_group_team . team playerId teamId eliminations_avg_per_10m deaths_avg_per_10m hero_damage_avg_per_10m healing_avg_per_10m ultimates_earned_avg_per_10m final_blows_avg_per_10m time_played_total count . 0 ATL | 85503 | 76980 | 157.793977 | 50.490675 | 62229.534763 | 29717.316325 | 39.633521 | 46.196669 | 542129.691046 | 10 | . 1 BOS | 19387 | 13206 | 41.831810 | 18.307436 | 22339.040042 | 168.870374 | 11.459645 | 13.611692 | 191882.761582 | 3 | . 2 CDH | 88084 | 76920 | 140.685496 | 54.735281 | 65018.505578 | 26011.366858 | 40.810739 | 46.357993 | 559672.649090 | 10 | . 3 DAL | 52627 | 45230 | 134.600049 | 53.890802 | 57832.731141 | 35178.978760 | 38.816079 | 41.888466 | 552438.204392 | 10 | . 4 FLA | 57758 | 30849 | 101.959246 | 36.758415 | 39591.619961 | 19430.429026 | 24.740010 | 28.107798 | 354423.819966 | 7 | . 5 GLA | 40743 | 30842 | 108.744030 | 30.418445 | 49108.905545 | 32116.483102 | 29.843228 | 32.485101 | 489485.111905 | 7 | . 6 GZC | 79962 | 84689 | 159.908158 | 48.163967 | 60829.784838 | 35702.505750 | 40.969978 | 55.463993 | 521269.576366 | 11 | . 7 HOU | 52327 | 49775 | 156.571056 | 57.913864 | 65358.076043 | 40454.474520 | 44.219426 | 48.886158 | 691999.044075 | 11 | . 8 HZS | 96271 | 84623 | 165.066886 | 51.045510 | 69069.351907 | 45746.309547 | 44.340600 | 48.711405 | 593674.115148 | 11 | . 9 LDN | 37085 | 26460 | 90.434697 | 26.671602 | 36982.082658 | 26613.463389 | 25.221463 | 24.639127 | 387766.475242 | 6 | . 10 NYE | 30576 | 26418 | 99.575626 | 24.138331 | 38255.470540 | 22063.733172 | 25.311343 | 29.345523 | 503951.861376 | 6 | . 11 PAR | 47618 | 46164 | 83.029907 | 33.794198 | 36440.938464 | 32370.046951 | 26.240437 | 24.154430 | 350082.354087 | 6 | . 12 PHI | 22529 | 22620 | 78.232372 | 27.090128 | 31924.133892 | 15973.523240 | 19.257253 | 23.235296 | 385046.356130 | 5 | . 13 SEO | 69111 | 44090 | 159.211885 | 42.290420 | 63252.547481 | 36999.399026 | 38.336938 | 44.785125 | 520537.701585 | 10 | . 14 SFS | 51678 | 39636 | 156.843551 | 40.146791 | 65342.859290 | 29724.106798 | 39.777856 | 52.126540 | 692734.994042 | 9 | . 15 SHD | 69402 | 44080 | 155.626680 | 49.550898 | 68986.768585 | 30876.841884 | 39.830569 | 53.093820 | 687468.766518 | 10 | . 16 TOR | 29461 | 38475 | 72.679144 | 27.565865 | 34029.883995 | 17312.415607 | 20.488759 | 25.676763 | 245498.218567 | 5 | . 17 VAL | 19483 | 13215 | 42.786018 | 15.343999 | 18309.481229 | 7423.094689 | 11.850342 | 14.706421 | 184116.543925 | 3 | . 18 VAN | 74664 | 69264 | 154.987665 | 40.865496 | 61391.476594 | 34382.552916 | 38.925150 | 46.432199 | 681441.973097 | 9 | . 19 WAS | 30441 | 30788 | 54.089034 | 18.851711 | 25049.833807 | 8057.760008 | 16.078539 | 18.847698 | 278076.144761 | 4 | . df_group_team[[&quot;team&quot;,&quot;count&quot;]] . team count . 0 ATL | 10 | . 1 BOS | 3 | . 2 CDH | 10 | . 3 DAL | 10 | . 4 FLA | 7 | . 5 GLA | 7 | . 6 GZC | 11 | . 7 HOU | 11 | . 8 HZS | 11 | . 9 LDN | 6 | . 10 NYE | 6 | . 11 PAR | 6 | . 12 PHI | 5 | . 13 SEO | 10 | . 14 SFS | 9 | . 15 SHD | 10 | . 16 TOR | 5 | . 17 VAL | 3 | . 18 VAN | 9 | . 19 WAS | 4 | . df_group_team.plot.bar(x = &quot;team&quot;, y = &quot;count&quot; ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5a3022b390&gt; . q1 = df_group_team[&quot;count&quot;].quantile(.25) q3 = df_group_team[&quot;count&quot;].quantile(.75) iqr = q3 - q1 lower_bound = q1 -(1.5 * iqr) upper_bound = q3 +(1.5 * iqr) print({&quot;Quartile 1&quot;: q1}) print({&quot;Quartile 3&quot;:q3}) print({&quot;IQR&quot;: iqr }) print({&quot;Lower Bound&quot; : lower_bound}) print({&quot;Upper Bound&quot; : upper_bound}) . {&#39;Quartile 1&#39;: 5.75} {&#39;Quartile 3&#39;: 10.0} {&#39;IQR&#39;: 4.25} {&#39;Lower Bound&#39;: -0.625} {&#39;Upper Bound&#39;: 16.375} . Although our teams don&#39;t have any outliers by the IQR, we probably should exclude the the Boston Uprising, LA Valliant, and Washington Justice as they don&#39;t have enough for a team . How Many Eliminations Do Each Team Get? . Let&#39;s find out which teams had the most eliminations per 10 minutes . df_group_team[&quot;Overall Average Eliminations Per 10m&quot;] = df_group_team[&quot;eliminations_avg_per_10m&quot;]/df_group_team[&quot;count&quot;] df_team_largest_elim = df_group_team[[&quot;team&quot;,&quot;Overall Average Eliminations Per 10m&quot;]] df_team_largest_elim.nlargest(20,&quot;Overall Average Eliminations Per 10m&quot;) . team Overall Average Eliminations Per 10m . 14 SFS | 17.427061 | . 18 VAN | 17.220852 | . 10 NYE | 16.595938 | . 13 SEO | 15.921189 | . 0 ATL | 15.779398 | . 12 PHI | 15.646474 | . 15 SHD | 15.562668 | . 5 GLA | 15.534861 | . 9 LDN | 15.072450 | . 8 HZS | 15.006081 | . 4 FLA | 14.565607 | . 6 GZC | 14.537105 | . 16 TOR | 14.535829 | . 17 VAL | 14.262006 | . 7 HOU | 14.233732 | . 2 CDH | 14.068550 | . 1 BOS | 13.943937 | . 11 PAR | 13.838318 | . 19 WAS | 13.522258 | . 3 DAL | 13.460005 | . df_group_team.plot.bar(x = &quot;team&quot;, y = &quot;Overall Average Eliminations Per 10m&quot; ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5a30163940&gt; . This looks pretty messy, let&#39;s calculate for any outliers . q1 = df_group_team[&quot;Overall Average Eliminations Per 10m&quot;].quantile(.25) q3 = df_group_team[&quot;Overall Average Eliminations Per 10m&quot;].quantile(.75) iqr = q3 - q1 lower_bound = q1 -(1.5 * iqr) upper_bound = q3 +(1.5 * iqr) print({&quot;Quartile 1&quot;: q1}) print({&quot;Quartile 3&quot;:q3}) print({&quot;IQR&quot;: iqr }) print({&quot;Lower Bound&quot; : lower_bound}) print({&quot;Upper Bound&quot; : upper_bound}) . {&#39;Quartile 1&#39;: 14.192436660434511} {&#39;Quartile 3&#39;: 15.679705219204635} {&#39;IQR&#39;: 1.4872685587701238} {&#39;Lower Bound&#39;: 11.961533822279325} {&#39;Upper Bound&#39;: 17.91060805735982} . How Much Damage is Dealt by Each Team? . df_group_team[&quot;Overall Average Damage Dealt per 10m&quot;] = df_group_team[&quot;hero_damage_avg_per_10m&quot;]/df_group_team[&quot;count&quot;] df_team_largest_damage = df_group_team[[&quot;team&quot;,&quot;Overall Average Damage Dealt per 10m&quot;]] df_team_largest_damage.nlargest(20,&quot;Overall Average Damage Dealt per 10m&quot;) . team Overall Average Damage Dealt per 10m . 1 BOS | 7446.346681 | . 14 SFS | 7260.317699 | . 5 GLA | 7015.557935 | . 15 SHD | 6898.676859 | . 18 VAN | 6821.275177 | . 16 TOR | 6805.976799 | . 2 CDH | 6501.850558 | . 12 PHI | 6384.826778 | . 10 NYE | 6375.911757 | . 13 SEO | 6325.254748 | . 8 HZS | 6279.031992 | . 19 WAS | 6262.458452 | . 0 ATL | 6222.953476 | . 9 LDN | 6163.680443 | . 17 VAL | 6103.160410 | . 11 PAR | 6073.489744 | . 7 HOU | 5941.643277 | . 3 DAL | 5783.273114 | . 4 FLA | 5655.945709 | . 6 GZC | 5529.980440 | . df_team_largest_damage.plot.bar(x = &quot;team&quot;, y = &quot;Overall Average Damage Dealt per 10m&quot; ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5a3010a198&gt; . Surpisingly enough, Boston seems to have a higher damage dealt dispite having less players. This is odd because it wasn&#39;t an issue for eliminations. . How Many Deaths is Recieved By Each Team? . df_group_team[&quot;Overall Average Deaths Per 10m&quot;] = df_group_team[&quot;deaths_avg_per_10m&quot;]/df_group_team[&quot;count&quot;] df_team_smallest_death = df_group_team[[&quot;team&quot;,&quot;Overall Average Deaths Per 10m&quot;]] df_team_smallest_death.nsmallest(20,&quot;Overall Average Deaths Per 10m&quot;) . team Overall Average Deaths Per 10m . 10 NYE | 4.023055 | . 13 SEO | 4.229042 | . 5 GLA | 4.345492 | . 6 GZC | 4.378542 | . 9 LDN | 4.445267 | . 14 SFS | 4.460755 | . 18 VAN | 4.540611 | . 8 HZS | 4.640501 | . 19 WAS | 4.712928 | . 15 SHD | 4.955090 | . 0 ATL | 5.049067 | . 17 VAL | 5.114666 | . 4 FLA | 5.251202 | . 7 HOU | 5.264897 | . 3 DAL | 5.389080 | . 12 PHI | 5.418026 | . 2 CDH | 5.473528 | . 16 TOR | 5.513173 | . 11 PAR | 5.632366 | . 1 BOS | 6.102479 | . df_team_smallest_death.plot.bar(x = &quot;team&quot;, y = &quot;Overall Average Deaths Per 10m&quot; ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f5a2fcee6d8&gt; . Linear Regression? . from sklearn.linear_model import LinearRegression . model = LinearRegression() model = model.fit(df[[&quot;eliminations_avg_per_10m&quot;,&quot;deaths_avg_per_10m&quot;,&quot;hero_damage_avg_per_10m&quot;, &quot;healing_avg_per_10m&quot;]], df[&quot;ultimates_earned_avg_per_10m&quot;]) .",
            "url": "https://progressedd.github.io/blog/2020/02/09/overwatch-pro-analysis.html",
            "relUrl": "/2020/02/09/overwatch-pro-analysis.html",
            "date": " • Feb 9, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://progressedd.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About Me . Hello there, I am Edward Tang, I have passion for technology and Data Science. My passion for the field wasn’t fully realized until colleagues identified Data Science practices in my work as an undergraduate research assistant. As a research assistant, I processed motion capture recordings, motion capture model development, and data analysis. . In my spare time, I help first time PC builders by tailoring parts for their workloads and troubleshoot computer issues on community chat servers and tech forums. As a Server Moderator for Illini Esports, I assist the admin team primarily by researching and implementing bots that enhance the experience and security for over 2,000 users. My other duties include working with the team to draft the constitution for our organization, as well as, coordinating server changes with community events led by community leaders. . About This Blog . The primary purpose of this blog is to document my experiences working on Data Science and Technology projects. I had previously shared links to notebooks on Github, but I encountered limitations that hindered some of my visualizations and explanations. . Contacting Me . I can be reached by any of the social media links below. .",
          "url": "https://progressedd.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://progressedd.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}